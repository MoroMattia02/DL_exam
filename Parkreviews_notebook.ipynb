{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Project**\n",
        "\n",
        "Moro Mattia\n",
        "The Notebook will implement an algorithm of sentimen analysis to predict the rating of the Park based on the review and other features. This project was created as exam of the 'Deep learning' module from the 'Machine Learning and Deep learning' course, successfully passed with evaluation 29/30 (30/30 in the total exam, considering also Machine Learning)."
      ],
      "metadata": {
        "id": "JofYXkn9lijH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk513xHsKbtY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Embedding\n",
        "from keras.layers.core import Dense, Activation\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) From the raw data to the input(s)**\n",
        "\n",
        "This was the first request (how to design the input layer). However, before that, it is necessarily to cover some adequate preprocessing steps.\n",
        "\n",
        "First of all, we import the dataset from GitHub (the file was zipped because too big)."
      ],
      "metadata": {
        "id": "VT5iPNdrm0J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/MoroMattia02/DL_exam/raw/main/parkReviews.zip'\n",
        "response = requests.get(url)\n",
        "zip_file = io.BytesIO(response.content)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    csv_filename = zip_ref.namelist()[0]\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "df_pre = pd.read_csv(f'/content/{csv_filename}',on_bad_lines='skip',encoding='latin-1')"
      ],
      "metadata": {
        "id": "2AhxfN2vO5Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains approximately 42000 reviews with 5400000 total words and 87000 unique ones. These numbers can't clearly be handled from this simple colab notebook. For this reason, only a subset of the dataset was taken.\n",
        "Before the sampling, i applied another filter. I reflected on the fact that, if i'm forced to take a subset of the dataset, and the most informative column has as mean approximately 130, the best choice was to first filter out 'outliers'. In fact, before sampling i created a function 'count_words' (is approximately correcrt, the real function for counting valid words is defined below, but it was too heavy to compute on 5400000 items), thanks to which i created a column with the number of words for each review. Then i took only reviews with words from 150 to 170, by predicting a loss of words of more or less 30 without punctuation and other non valid tokens.\n",
        "Then, for the sampling, after trying to run the network with different numbers of samples, 700 was the most reasonable trade-off between computational complexity and (still low) performance. In conclusion, a random sample (without replacement obviously) of 700 elements was taken."
      ],
      "metadata": {
        "id": "Dgz5b0Dyn-NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text):\n",
        "    words = text.split()  # splits the text at space\n",
        "    return len(words)\n",
        "\n",
        "df_pre['word_count'] = df_pre['Review_Text'].apply(lambda x: count_words(x)) #apply to the column\n",
        "\n",
        "\n",
        "df_pres = df_pre.loc[(df_pre['word_count'] >= 150) & (df_pre['word_count'] <= 170)] #filter for number of words\n",
        "df = df_pres.sample(450, replace=False,random_state=1) #sampling"
      ],
      "metadata": {
        "id": "LUyF1KrCj9DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the dataset is less heavy i can create the function to get the real words, and then filter the colum of reviews. To be precise, i repeat the counting process after the filtering."
      ],
      "metadata": {
        "id": "uJqYMR7CwueW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def get_words(text):\n",
        "  text = text.replace('--', ' ')\n",
        "  # split into tokens by white space\n",
        "  words = text.split()\n",
        "  # remove punctuation from each token\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  words = [w.translate(table) for w in words]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  # make lower case\n",
        "  words = [word.lower() for word in words]\n",
        "  return words\n",
        "\n",
        "def filter_words(df, text_column, unique_word_list):\n",
        "    filtered_column = df[text_column].apply(lambda text: ' '.join(word for word in text.split() if word in unique_word_list))\n",
        "    df[text_column] = filtered_column\n",
        "    return df\n",
        "\n",
        "text = df['Review_Text'].str.cat(sep = \" \")\n",
        "prt = get_words(text)\n",
        "unique_words = np.unique(prt)\n",
        "\n",
        "df_valid = filter_words(df, 'Review_Text', unique_words)\n",
        "df_valid"
      ],
      "metadata": {
        "id": "BFPwAQvOwjPg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "9f897199-add0-4399-9318-5d719a7bf9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Review_ID  Rating Year_Month Reviewer_Location  \\\n",
              "32236  474897517       5    2016-11           Ireland   \n",
              "17940  302685040       5     2015-8     United States   \n",
              "21184  225785637       5     2014-8            Canada   \n",
              "20356  241984006       4    2014-11            Canada   \n",
              "38404  201442095       1    missing    United Kingdom   \n",
              "...          ...     ...        ...               ...   \n",
              "42259   58409261       1    missing    United Kingdom   \n",
              "31270  531328489       3    missing           Ireland   \n",
              "22844  188966355       4    2013-12     United States   \n",
              "12068  528081243       4     2017-9     United States   \n",
              "22598  194295017       5     2014-2            Canada   \n",
              "\n",
              "                                             Review_Text  \\\n",
              "32236  visit every last year for my birthday as was a...   \n",
              "17940  for pete know what getting into when you go bu...   \n",
              "21184  the prrice keeps going up but how can you not ...   \n",
              "20356  time being to the the racing was very fun and ...   \n",
              "38404  loads of others rides kept closing for the mid...   \n",
              "...                                                  ...   \n",
              "42259  had a great time midweek untill my year old da...   \n",
              "31270  from to but the park was too busy at the the g...   \n",
              "22844  have been to many but every time we it seems t...   \n",
              "12068  the two parks for a total of during the celebr...   \n",
              "22598  time at hrs each day we were went and thought ...   \n",
              "\n",
              "                      Branch  word_count  \n",
              "32236       Disneyland_Paris         169  \n",
              "17940  Disneyland_California         165  \n",
              "21184  Disneyland_California         156  \n",
              "20356  Disneyland_California         155  \n",
              "38404       Disneyland_Paris         153  \n",
              "...                      ...         ...  \n",
              "42259       Disneyland_Paris         155  \n",
              "31270       Disneyland_Paris         151  \n",
              "22844  Disneyland_California         156  \n",
              "12068  Disneyland_California         160  \n",
              "22598  Disneyland_California         154  \n",
              "\n",
              "[450 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-1313abfd-c0cb-40d6-8fc1-1506ec9222c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Year_Month</th>\n",
              "      <th>Reviewer_Location</th>\n",
              "      <th>Review_Text</th>\n",
              "      <th>Branch</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32236</th>\n",
              "      <td>474897517</td>\n",
              "      <td>5</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>visit every last year for my birthday as was a...</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17940</th>\n",
              "      <td>302685040</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-8</td>\n",
              "      <td>United States</td>\n",
              "      <td>for pete know what getting into when you go bu...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21184</th>\n",
              "      <td>225785637</td>\n",
              "      <td>5</td>\n",
              "      <td>2014-8</td>\n",
              "      <td>Canada</td>\n",
              "      <td>the prrice keeps going up but how can you not ...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20356</th>\n",
              "      <td>241984006</td>\n",
              "      <td>4</td>\n",
              "      <td>2014-11</td>\n",
              "      <td>Canada</td>\n",
              "      <td>time being to the the racing was very fun and ...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38404</th>\n",
              "      <td>201442095</td>\n",
              "      <td>1</td>\n",
              "      <td>missing</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>loads of others rides kept closing for the mid...</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42259</th>\n",
              "      <td>58409261</td>\n",
              "      <td>1</td>\n",
              "      <td>missing</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>had a great time midweek untill my year old da...</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31270</th>\n",
              "      <td>531328489</td>\n",
              "      <td>3</td>\n",
              "      <td>missing</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>from to but the park was too busy at the the g...</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22844</th>\n",
              "      <td>188966355</td>\n",
              "      <td>4</td>\n",
              "      <td>2013-12</td>\n",
              "      <td>United States</td>\n",
              "      <td>have been to many but every time we it seems t...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12068</th>\n",
              "      <td>528081243</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-9</td>\n",
              "      <td>United States</td>\n",
              "      <td>the two parks for a total of during the celebr...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22598</th>\n",
              "      <td>194295017</td>\n",
              "      <td>5</td>\n",
              "      <td>2014-2</td>\n",
              "      <td>Canada</td>\n",
              "      <td>time at hrs each day we were went and thought ...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>450 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1313abfd-c0cb-40d6-8fc1-1506ec9222c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-07cf49a6-cec1-44e5-bff4-a297ce2eb600\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07cf49a6-cec1-44e5-bff4-a297ce2eb600')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-07cf49a6-cec1-44e5-bff4-a297ce2eb600 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1313abfd-c0cb-40d6-8fc1-1506ec9222c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1313abfd-c0cb-40d6-8fc1-1506ec9222c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.drop(['word_count'], axis=1)\n",
        "df_valid['word_count'] = df_valid['Review_Text'].apply(lambda x: count_words(x))"
      ],
      "metadata": {
        "id": "tZ2uCmBfktMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can print mean and standard deviation of the number of words."
      ],
      "metadata": {
        "id": "ue2F-7D2lihE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid['word_count'].std()"
      ],
      "metadata": {
        "id": "1A-h8mqxRfoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b7bc20-ed12-40bc-d23a-edcceff3c7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.452813808052614"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid['word_count'].mean()"
      ],
      "metadata": {
        "id": "OaAU4stvRkIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f9c49d-ecb8-42ef-bca3-99dafd7db891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124.33555555555556"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the sample mean is not too far from the population mean."
      ],
      "metadata": {
        "id": "zsOL1VBelu6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word Embeddings**\n",
        "\n",
        "The most accurate choice for word embedding would have been to train a word2vec on our own dictionary and then use that, or maybe to download the one from google. However, to remain adherent to what was seen in the course, i embedded the words as one-hot vectors.\n",
        "These are the steps that i followed:\n",
        "\n",
        "- get all the unique words (already done above in the code);\n",
        "- create a dictionary {word:index};\n",
        "- replace any index with the correspondent one-hot vector, using as length the number of unique words and placing the 1 at the position given by the dictionary;\n",
        "\n",
        "These are the steps to create the dictionary {word:vector}.\n",
        "\n"
      ],
      "metadata": {
        "id": "FcadJK1amKBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))\n",
        "print(len(unique_words))\n",
        "print(f\"Unique words:{unique_words[:20]}\")\n",
        "print(list(unique_word_index.items())[:20])"
      ],
      "metadata": {
        "id": "JuEfrkZlTFy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0caf0003-f026-4f16-db4c-5a7d04058c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5466\n",
            "Unique words:['a' 'aa' 'aaa' 'abandoned' 'ability' 'abit' 'able' 'about' 'above'\n",
            " 'abruptly' 'absent' 'absolute' 'absolutely' 'abt' 'abuse' 'abysmal' 'ac'\n",
            " 'accent' 'accept' 'acceptable']\n",
            "[('a', 0), ('aa', 1), ('aaa', 2), ('abandoned', 3), ('ability', 4), ('abit', 5), ('able', 6), ('about', 7), ('above', 8), ('abruptly', 9), ('absent', 10), ('absolute', 11), ('absolutely', 12), ('abt', 13), ('abuse', 14), ('abysmal', 15), ('ac', 16), ('accent', 17), ('accept', 18), ('acceptable', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unique_word_index)"
      ],
      "metadata": {
        "id": "14NeTDM2VYKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07c9226-6cba-494a-c82e-2e2e372b45dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'aa': 1, 'aaa': 2, 'abandoned': 3, 'ability': 4, 'abit': 5, 'able': 6, 'about': 7, 'above': 8, 'abruptly': 9, 'absent': 10, 'absolute': 11, 'absolutely': 12, 'abt': 13, 'abuse': 14, 'abysmal': 15, 'ac': 16, 'accent': 17, 'accept': 18, 'acceptable': 19, 'accepting': 20, 'access': 21, 'accessed': 22, 'accessible': 23, 'accident': 24, 'accommodate': 25, 'accommodated': 26, 'accommodating': 27, 'accommodation': 28, 'accomodation': 29, 'accomodations': 30, 'accompany': 31, 'accompanying': 32, 'accomplished': 33, 'according': 34, 'accordingly': 35, 'accounts': 36, 'accurate': 37, 'accustomed': 38, 'acknowledged': 39, 'across': 40, 'act': 41, 'acting': 42, 'actionget': 43, 'activities': 44, 'activity': 45, 'actor': 46, 'actors': 47, 'actual': 48, 'actually': 49, 'actuallyspace': 50, 'adc': 51, 'add': 52, 'added': 53, 'addition': 54, 'additional': 55, 'address': 56, 'addressing': 57, 'adds': 58, 'adequate': 59, 'adequately': 60, 'adjust': 61, 'admission': 62, 'admissions': 63, 'admit': 64, 'admusement': 65, 'adoptees': 66, 'adorable': 67, 'adorablethere': 68, 'adore': 69, 'adored': 70, 'adrenal': 71, 'adrenaline': 72, 'adult': 73, 'adults': 74, 'advance': 75, 'advantage': 76, 'advantages': 77, 'adventure': 78, 'adventureits': 79, 'adventureland': 80, 'adventurelandthe': 81, 'adventuremy': 82, 'adventures': 83, 'adventurethat': 84, 'adventurous': 85, 'advertise': 86, 'advertised': 87, 'advice': 88, 'advisable': 89, 'advise': 90, 'advised': 91, 'advisor': 92, 'aerosmith': 93, 'aesthetically': 94, 'affect': 95, 'afford': 96, 'affordable': 97, 'after': 98, 'afternoon': 99, 'afternoons': 100, 'afterwards': 101, 'again': 102, 'againalmost': 103, 'againi': 104, 'againif': 105, 'againour': 106, 'against': 107, 'againthere': 108, 'age': 109, 'aged': 110, 'agenda': 111, 'agents': 112, 'ages': 113, 'ageshad': 114, 'aggresive': 115, 'aggressive': 116, 'ago': 117, 'agodont': 118, 'agothe': 119, 'agreat': 120, 'agree': 121, 'agreed': 122, 'ahe': 123, 'ahead': 124, 'aid': 125, 'aimed': 126, 'air': 127, 'airport': 128, 'aka': 129, 'aladdins': 130, 'aladin': 131, 'albeit': 132, 'alert': 133, 'algae': 134, 'alice': 135, 'alices': 136, 'alienate': 137, 'aliens': 138, 'alighting': 139, 'alike': 140, 'alikewe': 141, 'alive': 142, 'all': 143, 'allare': 144, 'allergies': 145, 'allergy': 146, 'alleviated': 147, 'allocated': 148, 'alloted': 149, 'allover': 150, 'allow': 151, 'allowed': 152, 'allowing': 153, 'allows': 154, 'almost': 155, 'alone': 156, 'alonewe': 157, 'along': 158, 'alot': 159, 'already': 160, 'alright': 161, 'also': 162, 'alsofood': 163, 'alternative': 164, 'although': 165, 'alton': 166, 'always': 167, 'am': 168, 'amaze': 169, 'amazed': 170, 'amazing': 171, 'amazingalso': 172, 'amazingi': 173, 'amazingly': 174, 'amazingsure': 175, 'america': 176, 'american': 177, 'americana': 178, 'americans': 179, 'among': 180, 'amount': 181, 'ample': 182, 'amsterdam': 183, 'amused': 184, 'amusement': 185, 'amusements': 186, 'amusementscare': 187, 'an': 188, 'anaheim': 189, 'anaheims': 190, 'ancient': 191, 'and': 192, 'android': 193, 'aneheim': 194, 'angeles': 195, 'angered': 196, 'angry': 197, 'animals': 198, 'animated': 199, 'anime': 200, 'anna': 201, 'anniversary': 202, 'anniversaryi': 203, 'announced': 204, 'announcement': 205, 'announcements': 206, 'annoyed': 207, 'annoying': 208, 'annual': 209, 'another': 210, 'answer': 211, 'ant': 212, 'anticipate': 213, 'anticipating': 214, 'anticipation': 215, 'any': 216, 'anymore': 217, 'anyone': 218, 'anything': 219, 'anytime': 220, 'anyway': 221, 'anywaybest': 222, 'anyways': 223, 'anywaywe': 224, 'anywhere': 225, 'apart': 226, 'apologies': 227, 'apologize': 228, 'apology': 229, 'app': 230, 'apparent': 231, 'apparently': 232, 'appeal': 233, 'appealing': 234, 'appear': 235, 'appeared': 236, 'appears': 237, 'appetitethe': 238, 'apple': 239, 'applied': 240, 'apply': 241, 'appreciate': 242, 'appreciated': 243, 'approached': 244, 'appropriate': 245, 'appropriatethe': 246, 'approx': 247, 'approximate': 248, 'apps': 249, 'april': 250, 'aprk': 251, 'aquatica': 252, 'arabian': 253, 'arc': 254, 'are': 255, 'area': 256, 'areas': 257, 'areascarry': 258, 'aren': 259, 'arent': 260, 'areo': 261, 'argue': 262, 'ariel': 263, 'ariels': 264, 'ariseoverallfabulous': 265, 'armageddon': 266, 'arnt': 267, 'around': 268, 'aroundqueues': 269, 'aroundshows': 270, 'aroundsome': 271, 'arr': 272, 'arranged': 273, 'arrival': 274, 'arrive': 275, 'arrived': 276, 'arrivedsome': 277, 'arriving': 278, 'arrnaged': 279, 'art': 280, 'artifacts': 281, 'as': 282, 'asap': 283, 'ashamed': 284, 'asian': 285, 'asians': 286, 'aside': 287, 'asimo': 288, 'ask': 289, 'asked': 290, 'asking': 291, 'aspect': 292, 'aspects': 293, 'aspergers': 294, 'assisiting': 295, 'assistance': 296, 'assistant': 297, 'assit': 298, 'assume': 299, 'asterix': 300, 'astounding': 301, 'astro': 302, 'astroblasters': 303, 'at': 304, 'ate': 305, 'atleast': 306, 'atmosphere': 307, 'atop': 308, 'atraction': 309, 'attached': 310, 'attempt': 311, 'attempting': 312, 'attendance': 313, 'attendant': 314, 'attendants': 315, 'attended': 316, 'attending': 317, 'attention': 318, 'attentive': 319, 'attitude': 320, 'attraction': 321, 'attractionbecause': 322, 'attractions': 323, 'attractionsfood': 324, 'attractionsuse': 325, 'aud': 326, 'august': 327, 'auroras': 328, 'australia': 329, 'authorities': 330, 'authority': 331, 'autistic': 332, 'autograph': 333, 'autographs': 334, 'autopia': 335, 'auxiliary': 336, 'available': 337, 'availablestraight': 338, 'avengers': 339, 'avenue': 340, 'average': 341, 'avid': 342, 'avoid': 343, 'avoided': 344, 'await': 345, 'awaited': 346, 'awake': 347, 'aware': 348, 'away': 349, 'awaykeep': 350, 'aways': 351, 'awe': 352, 'awesome': 353, 'awesomehuge': 354, 'awful': 355, 'awfulthat': 356, 'awhile': 357, 'awsome': 358, 'b': 359, 'babies': 360, 'baby': 361, 'back': 362, 'backafter': 363, 'backheads': 364, 'backpack': 365, 'backwards': 366, 'bad': 367, 'badalthough': 368, 'badge': 369, 'badges': 370, 'badly': 371, 'bag': 372, 'bags': 373, 'bakery': 374, 'bale': 375, 'ball': 376, 'balloon': 377, 'balloons': 378, 'banana': 379, 'band': 380, 'bands': 381, 'bang': 382, 'bank': 383, 'banned': 384, 'banterthey': 385, 'bar': 386, 'barely': 387, 'barging': 388, 'barking': 389, 'barriers': 390, 'based': 391, 'basement': 392, 'bash': 393, 'basic': 394, 'basically': 395, 'basis': 396, 'bath': 397, 'bathroom': 398, 'bathrooms': 399, 'battery': 400, 'battles': 401, 'bay': 402, 'baymax': 403, 'bayou': 404, 'bazzars': 405, 'bc': 406, 'be': 407, 'beacause': 408, 'beach': 409, 'beaf': 410, 'bear': 411, 'beat': 412, 'beautiful': 413, 'beautifully': 414, 'beauty': 415, 'beautys': 416, 'bec': 417, 'became': 418, 'becasue': 419, 'because': 420, 'become': 421, 'becomes': 422, 'becoming': 423, 'bed': 424, 'beddy': 425, 'bedrooms': 426, 'beds': 427, 'been': 428, 'before': 429, 'beforehand': 430, 'beforeif': 431, 'beforethe': 432, 'begging': 433, 'beginning': 434, 'begins': 435, 'behaviour': 436, 'behind': 437, 'being': 438, 'belgium': 439, 'believe': 440, 'believed': 441, 'bell': 442, 'belle': 443, 'below': 444, 'ben': 445, 'bench': 446, 'bend': 447, 'benefit': 448, 'berry': 449, 'berth': 450, 'beside': 451, 'besides': 452, 'best': 453, 'bestheres': 454, 'bet': 455, 'better': 456, 'between': 457, 'beverages': 458, 'beware': 459, 'beyond': 460, 'bf': 461, 'bibbidi': 462, 'bibidy': 463, 'bible': 464, 'bid': 465, 'big': 466, 'bigger': 467, 'biggerconclusion': 468, 'biggest': 469, 'bill': 470, 'billion': 471, 'billions': 472, 'bills': 473, 'birth': 474, 'birthday': 475, 'birthdays': 476, 'bit': 477, 'bite': 478, 'bites': 479, 'bizarre': 480, 'black': 481, 'blame': 482, 'blast': 483, 'blasters': 484, 'blastersunfortunately': 485, 'blastrock': 486, 'blatantly': 487, 'bless': 488, 'blessed': 489, 'blessing': 490, 'blind': 491, 'bliss': 492, 'block': 493, 'blocked': 494, 'blocking': 495, 'blocks': 496, 'bloody': 497, 'blowing': 498, 'blown': 499, 'blu': 500, 'blue': 501, 'board': 502, 'boarded': 503, 'boardwhich': 504, 'boat': 505, 'boats': 506, 'bobbed': 507, 'bobidy': 508, 'bobsled': 509, 'bobsleds': 510, 'bombard': 511, 'bond': 512, 'bonfire': 513, 'bonus': 514, 'book': 515, 'booked': 516, 'booking': 517, 'books': 518, 'booth': 519, 'booths': 520, 'bored': 521, 'boring': 522, 'born': 523, 'both': 524, 'bother': 525, 'bothered': 526, 'bottle': 527, 'bottled': 528, 'bottlenecks': 529, 'bottles': 530, 'bottleseverything': 531, 'bottleshope': 532, 'bottlesour': 533, 'bottom': 534, 'bought': 535, 'bounce': 536, 'bout': 537, 'boutique': 538, 'bowled': 539, 'bowlyou': 540, 'box': 541, 'boxed': 542, 'boy': 543, 'boybad': 544, 'boyfriend': 545, 'boys': 546, 'brace': 547, 'brainchild': 548, 'brainstorm': 549, 'brand': 550, 'break': 551, 'breakdowns': 552, 'breakfast': 553, 'breakfastall': 554, 'breaking': 555, 'breakmany': 556, 'breaks': 557, 'breath': 558, 'breathe': 559, 'breathing': 560, 'breathtaking': 561, 'breezewe': 562, 'briar': 563, 'bricks': 564, 'bright': 565, 'brilliant': 566, 'brimmed': 567, 'bring': 568, 'bringing': 569, 'brings': 570, 'brochure': 571, 'broke': 572, 'broken': 573, 'broods': 574, 'brookhurst': 575, 'brother': 576, 'brothers': 577, 'brought': 578, 'brownie': 579, 'brrrbut': 580, 'brussels': 581, 'brutal': 582, 'btw': 583, 'bubble': 584, 'buck': 585, 'bucket': 586, 'bucks': 587, 'budget': 588, 'budha': 589, 'buds': 590, 'buffalo': 591, 'buffet': 592, 'buffets': 593, 'bugets': 594, 'buggies': 595, 'buggy': 596, 'bugs': 597, 'build': 598, 'building': 599, 'buildings': 600, 'built': 601, 'bull': 602, 'bullet': 603, 'bummer': 604, 'bump': 605, 'bunch': 606, 'burger': 607, 'burgers': 608, 'buried': 609, 'burning': 610, 'burns': 611, 'bus': 612, 'buses': 613, 'busier': 614, 'busiest': 615, 'business': 616, 'busloads': 617, 'buss': 618, 'bussed': 619, 'busses': 620, 'busty': 621, 'busy': 622, 'but': 623, 'butts': 624, 'buttschipped': 625, 'buttsfirst': 626, 'buy': 627, 'buying': 628, 'buyingbut': 629, 'buzz': 630, 'by': 631, 'bye': 632, 'bypass': 633, 'c': 634, 'ca': 635, 'cab': 636, 'cabane': 637, 'cabins': 638, 'cafe': 639, 'cake': 640, 'cakes': 641, 'caketaste': 642, 'cal': 643, 'calendar': 644, 'calif': 645, 'california': 646, 'call': 647, 'called': 648, 'calling': 649, 'calls': 650, 'calm': 651, 'calmost': 652, 'came': 653, 'camera': 654, 'cameras': 655, 'camped': 656, 'can': 657, 'canada': 658, 'cancel': 659, 'cancellations': 660, 'cancelled': 661, 'candlelight': 662, 'candles': 663, 'candy': 664, 'cane': 665, 'cannot': 666, 'cant': 667, 'cantonese': 668, 'canyon': 669, 'cap': 670, 'capacity': 671, 'cape': 672, 'capistrano': 673, 'capitol': 674, 'capt': 675, 'captain': 676, 'capture': 677, 'captured': 678, 'car': 679, 'carand': 680, 'card': 681, 'cardi': 682, 'care': 683, 'cared': 684, 'careen': 685, 'careful': 686, 'carefully': 687, 'cares': 688, 'caribbean': 689, 'caribean': 690, 'carland': 691, 'carnaval': 692, 'carols': 693, 'carousel': 694, 'carousels': 695, 'carriage': 696, 'carriages': 697, 'carribbean': 698, 'carribean': 699, 'carribeanthe': 700, 'carrier': 701, 'carry': 702, 'carrying': 703, 'cars': 704, 'carsland': 705, 'carte': 706, 'cartoon': 707, 'carts': 708, 'case': 709, 'cast': 710, 'castle': 711, 'castles': 712, 'catch': 713, 'catching': 714, 'catchy': 715, 'cater': 716, 'catered': 717, 'catering': 718, 'caters': 719, 'cattle': 720, 'caught': 721, 'cause': 722, 'caused': 723, 'causing': 724, 'cautious': 725, 'cave': 726, 'cdg': 727, 'celcius': 728, 'celebrate': 729, 'celebrated': 730, 'celebrating': 731, 'celebration': 732, 'celebrations': 733, 'celebrationscost': 734, 'celiacs': 735, 'cell': 736, 'cent': 737, 'center': 738, 'central': 739, 'centre': 740, 'centres': 741, 'ceremony': 742, 'certain': 743, 'certainly': 744, 'chair': 745, 'chalet': 746, 'challenge': 747, 'chams': 748, 'chance': 749, 'chances': 750, 'change': 751, 'changed': 752, 'changing': 753, 'chaotic': 754, 'character': 755, 'characteri': 756, 'characters': 757, 'characterswe': 758, 'charecters': 759, 'charectors': 760, 'charge': 761, 'charged': 762, 'charging': 763, 'charm': 764, 'charming': 765, 'chat': 766, 'chateletif': 767, 'chatted': 768, 'chatter': 769, 'cheap': 770, 'cheaper': 771, 'cheaperfor': 772, 'check': 773, 'checked': 774, 'checking': 775, 'checks': 776, 'cheer': 777, 'cheerful': 778, 'cheery': 779, 'chef': 780, 'cherish': 781, 'chessy': 782, 'chest': 783, 'chested': 784, 'chewing': 785, 'cheyenne': 786, 'chicken': 787, 'child': 788, 'childhood': 789, 'childhoodi': 790, 'childish': 791, 'childonline': 792, 'children': 793, 'childrenif': 794, 'childrens': 795, 'childrenthe': 796, 'childrenvery': 797, 'childs': 798, 'chill': 799, 'chilliy': 800, 'chilly': 801, 'china': 802, 'chinese': 803, 'chips': 804, 'chocolate': 805, 'choice': 806, 'choices': 807, 'choicethe': 808, 'choir': 809, 'choose': 810, 'chorse': 811, 'chose': 812, 'christ': 813, 'christmas': 814, 'christmaswell': 815, 'christmasy': 816, 'chuck': 817, 'churros': 818, 'churrosyou': 819, 'cigarette': 820, 'cigarettes': 821, 'cinderella': 822, 'cinema': 823, 'circle': 824, 'circuit': 825, 'citizen': 826, 'citizens': 827, 'city': 828, 'cityin': 829, 'claimed': 830, 'claiming': 831, 'clarify': 832, 'class': 833, 'classic': 834, 'classifiedoverall': 835, 'clean': 836, 'cleaned': 837, 'cleaner': 838, 'cleaniliess': 839, 'cleaning': 840, 'cleaningthe': 841, 'cleanliness': 842, 'cleanq': 843, 'cleanup': 844, 'cleanyes': 845, 'clear': 846, 'clearly': 847, 'clever': 848, 'click': 849, 'clicked': 850, 'climate': 851, 'climbed': 852, 'climbing': 853, 'clock': 854, 'close': 855, 'closeat': 856, 'closebutit': 857, 'closed': 858, 'closeddespite': 859, 'closedindiana': 860, 'closer': 861, 'closes': 862, 'closest': 863, 'closeswe': 864, 'closing': 865, 'closure': 866, 'closures': 867, 'clothes': 868, 'clothing': 869, 'clouds': 870, 'cloudy': 871, 'club': 872, 'clubhouse': 873, 'clue': 874, 'clutch': 875, 'cm': 876, 'cms': 877, 'co': 878, 'coach': 879, 'coast': 880, 'coaster': 881, 'coasters': 882, 'coat': 883, 'coats': 884, 'cocoa': 885, 'cod': 886, 'code': 887, 'coffee': 888, 'coffees': 889, 'coincided': 890, 'coke': 891, 'cokes': 892, 'cold': 893, 'colder': 894, 'colleague': 895, 'collect': 896, 'collected': 897, 'collecteveryone': 898, 'college': 899, 'color': 900, 'colored': 901, 'colorful': 902, 'colors': 903, 'colour': 904, 'colourfulnobody': 905, 'colouring': 906, 'combination': 907, 'come': 908, 'comes': 909, 'comfortable': 910, 'comfortably': 911, 'comfy': 912, 'comics': 913, 'coming': 914, 'comment': 915, 'commented': 916, 'comments': 917, 'commercial': 918, 'commercialised': 919, 'commitment': 920, 'common': 921, 'communicating': 922, 'comp': 923, 'compact': 924, 'compaired': 925, 'company': 926, 'comparable': 927, 'comparative': 928, 'comparatively': 929, 'compare': 930, 'compared': 931, 'comparing': 932, 'comparison': 933, 'comparisonbut': 934, 'comparisons': 935, 'compensation': 936, 'compete': 937, 'complain': 938, 'complained': 939, 'complaining': 940, 'complaint': 941, 'complaints': 942, 'complete': 943, 'completely': 944, 'compliants': 945, 'complicated': 946, 'complimentary': 947, 'computer': 948, 'con': 949, 'concentrated': 950, 'concept': 951, 'concerned': 952, 'concernwe': 953, 'concert': 954, 'concreate': 955, 'concurrently': 956, 'conditioned': 957, 'conditioning': 958, 'conditions': 959, 'confirmed': 960, 'conflicting': 961, 'confused': 962, 'confusing': 963, 'congestion': 964, 'connect': 965, 'connected': 966, 'connecting': 967, 'cons': 968, 'consider': 969, 'considerably': 970, 'consideration': 971, 'considered': 972, 'considering': 973, 'consisted': 974, 'consolation': 975, 'constant': 976, 'constantly': 977, 'construction': 978, 'consuming': 979, 'containers': 980, 'continue': 981, 'continued': 982, 'continuely': 983, 'continuously': 984, 'contrast': 985, 'control': 986, 'controlwe': 987, 'conveniences': 988, 'convenient': 989, 'convert': 990, 'convinced': 991, 'cooking': 992, 'cool': 993, 'cooldrink': 994, 'cooler': 995, 'coolers': 996, 'coolest': 997, 'coordinates': 998, 'cope': 999, 'copy': 1000, 'cor': 1001, 'coralled': 1002, 'cordoned': 1003, 'cordoning': 1004, 'corn': 1005, 'corner': 1006, 'cornering': 1007, 'corporate': 1008, 'corporation': 1009, 'correct': 1010, 'cost': 1011, 'costed': 1012, 'costly': 1013, 'costs': 1014, 'costume': 1015, 'costumes': 1016, 'cosymy': 1017, 'could': 1018, 'couldnt': 1019, 'coulourfull': 1020, 'count': 1021, 'counter': 1022, 'counterpart': 1023, 'counters': 1024, 'countless': 1025, 'countries': 1026, 'country': 1027, 'couple': 1028, 'coupled': 1029, 'couples': 1030, 'couplesavoid': 1031, 'coupon': 1032, 'coupons': 1033, 'course': 1034, 'court': 1035, 'courteous': 1036, 'cousin': 1037, 'cousins': 1038, 'cover': 1039, 'covered': 1040, 'coverit': 1041, 'cracked': 1042, 'cracker': 1043, 'cram': 1044, 'crammed': 1045, 'cramming': 1046, 'cranking': 1047, 'crapbuy': 1048, 'crappy': 1049, 'craving': 1050, 'crazy': 1051, 'cream': 1052, 'creamwe': 1053, 'create': 1054, 'created': 1055, 'creates': 1056, 'creation': 1057, 'credit': 1058, 'creed': 1059, 'crew': 1060, 'cried': 1061, 'critic': 1062, 'critical': 1063, 'criticism': 1064, 'crockett': 1065, 'cross': 1066, 'crossed': 1067, 'crossing': 1068, 'crowd': 1069, 'crowde': 1070, 'crowded': 1071, 'crowdedness': 1072, 'crowdedplenty': 1073, 'crowdedstaying': 1074, 'crowdedthe': 1075, 'crowding': 1076, 'crowds': 1077, 'crowdshave': 1078, 'crowdso': 1079, 'cruise': 1080, 'cruising': 1081, 'crush': 1082, 'crushs': 1083, 'crying': 1084, 'cuase': 1085, 'cud': 1086, 'cue': 1087, 'cued': 1088, 'cues': 1089, 'cuisine': 1090, 'culmination': 1091, 'cultural': 1092, 'culture': 1093, 'cup': 1094, 'cupboard': 1095, 'cups': 1096, 'curb': 1097, 'curious': 1098, 'currency': 1099, 'current': 1100, 'currently': 1101, 'curt': 1102, 'cushioned': 1103, 'customer': 1104, 'customers': 1105, 'cut': 1106, 'cutbacks': 1107, 'cute': 1108, 'cutting': 1109, 'cynic': 1110, 'cynicism': 1111, 'd': 1112, 'dad': 1113, 'dads': 1114, 'daily': 1115, 'daisy': 1116, 'dance': 1117, 'danced': 1118, 'dancing': 1119, 'dark': 1120, 'darn': 1121, 'darth': 1122, 'date': 1123, 'dated': 1124, 'datei': 1125, 'dates': 1126, 'datesthe': 1127, 'daugher': 1128, 'daughter': 1129, 'daughters': 1130, 'daughterthings': 1131, 'daunting': 1132, 'davey': 1133, 'davy': 1134, 'day': 1135, 'dayall': 1136, 'daybig': 1137, 'daydisney': 1138, 'dayif': 1139, 'dayit': 1140, 'dayp': 1141, 'days': 1142, 'daysif': 1143, 'daysmonday': 1144, 'daytime': 1145, 'daywill': 1146, 'dayyou': 1147, 'dead': 1148, 'deal': 1149, 'dealer': 1150, 'deals': 1151, 'dealt': 1152, 'death': 1153, 'debating': 1154, 'dec': 1155, 'decade': 1156, 'decaf': 1157, 'december': 1158, 'decent': 1159, 'deceptive': 1160, 'decide': 1161, 'decided': 1162, 'decision': 1163, 'decked': 1164, 'decker': 1165, 'deconly': 1166, 'decor': 1167, 'decorated': 1168, 'decorations': 1169, 'decorationsthe': 1170, 'dedicated': 1171, 'deduce': 1172, 'defeats': 1173, 'defenatly': 1174, 'deffo': 1175, 'defiantly': 1176, 'defiling': 1177, 'definately': 1178, 'definitely': 1179, 'defnitely': 1180, 'degrees': 1181, 'delayed': 1182, 'delays': 1183, 'delicious': 1184, 'delight': 1185, 'delighted': 1186, 'delightful': 1187, 'deliver': 1188, 'delivered': 1189, 'deliveri': 1190, 'delivers': 1191, 'deluxe': 1192, 'denied': 1193, 'dennis': 1194, 'depend': 1195, 'depending': 1196, 'depends': 1197, 'depicting': 1198, 'depressing': 1199, 'dept': 1200, 'derisory': 1201, 'des': 1202, 'describe': 1203, 'described': 1204, 'descriptions': 1205, 'deserves': 1206, 'designated': 1207, 'designed': 1208, 'desire': 1209, 'desired': 1210, 'despite': 1211, 'destination': 1212, 'destinations': 1213, 'destroy': 1214, 'destroyed': 1215, 'detail': 1216, 'detailed': 1217, 'detailing': 1218, 'details': 1219, 'detectors': 1220, 'deter': 1221, 'determine': 1222, 'determined': 1223, 'deterrent': 1224, 'develop': 1225, 'development': 1226, 'diamond': 1227, 'dianey': 1228, 'diapers': 1229, 'did': 1230, 'didn': 1231, 'didnt': 1232, 'die': 1233, 'died': 1234, 'diego': 1235, 'dieny': 1236, 'diet': 1237, 'dietary': 1238, 'diff': 1239, 'difference': 1240, 'differencein': 1241, 'differences': 1242, 'different': 1243, 'difficult': 1244, 'difficulties': 1245, 'difficulty': 1246, 'digital': 1247, 'diminishes': 1248, 'dineyland': 1249, 'ding': 1250, 'dining': 1251, 'dinner': 1252, 'dinnerthe': 1253, 'dinseyland': 1254, 'dipped': 1255, 'direct': 1256, 'directing': 1257, 'directions': 1258, 'directly': 1259, 'dirty': 1260, 'disabilities': 1261, 'disability': 1262, 'disabled': 1263, 'disagree': 1264, 'disappeared': 1265, 'disappoint': 1266, 'disappointed': 1267, 'disappointedbe': 1268, 'disappointedsince': 1269, 'disappointent': 1270, 'disappointing': 1271, 'disappointingalways': 1272, 'disappointingmy': 1273, 'disappointingthe': 1274, 'disappointment': 1275, 'disappointmentnot': 1276, 'disappointmentwe': 1277, 'disaster': 1278, 'discarding': 1279, 'disconnected': 1280, 'discount': 1281, 'discounts': 1282, 'discover': 1283, 'discovered': 1284, 'disgusted': 1285, 'disgusting': 1286, 'dishes': 1287, 'disinterested': 1288, 'disleyland': 1289, 'dislike': 1290, 'dislikes': 1291, 'dismal': 1292, 'disneland': 1293, 'disnexperience': 1294, 'disney': 1295, 'disneyana': 1296, 'disneyland': 1297, 'disneylandcom': 1298, 'disneylandgreat': 1299, 'disneylandi': 1300, 'disneylandit': 1301, 'disneylandor': 1302, 'disneylands': 1303, 'disneylandsthe': 1304, 'disneylandtheres': 1305, 'disneyness': 1306, 'disneyour': 1307, 'disneypros': 1308, 'disneys': 1309, 'disneyswho': 1310, 'disneythe': 1311, 'disneyworld': 1312, 'disorganised': 1313, 'displace': 1314, 'display': 1315, 'displays': 1316, 'disposable': 1317, 'dissapointed': 1318, 'dissapointedtips': 1319, 'dissapointing': 1320, 'dissapointmentwhen': 1321, 'dissappointed': 1322, 'distance': 1323, 'distant': 1324, 'distinctly': 1325, 'distracted': 1326, 'distracting': 1327, 'distributed': 1328, 'district': 1329, 'disturbing': 1330, 'diversity': 1331, 'dj': 1332, 'dl': 1333, 'dland': 1334, 'dlcal': 1335, 'dlp': 1336, 'dlptruly': 1337, 'do': 1338, 'dodging': 1339, 'does': 1340, 'doesnt': 1341, 'dog': 1342, 'dogs': 1343, 'doing': 1344, 'doingthe': 1345, 'dole': 1346, 'dollar': 1347, 'dollars': 1348, 'domake': 1349, 'don': 1350, 'donald': 1351, 'donaldthe': 1352, 'done': 1353, 'donejungle': 1354, 'dont': 1355, 'doom': 1356, 'door': 1357, 'dope': 1358, 'dos': 1359, 'dose': 1360, 'double': 1361, 'doubt': 1362, 'doughnut': 1363, 'dowith': 1364, 'down': 1365, 'downdefinitely': 1366, 'downdisappointed': 1367, 'downgraded': 1368, 'downi': 1369, 'download': 1370, 'downloaded': 1371, 'downmain': 1372, 'downpour': 1373, 'downright': 1374, 'downside': 1375, 'downsidesthe': 1376, 'downsize': 1377, 'downtown': 1378, 'dozen': 1379, 'dozens': 1380, 'drab': 1381, 'drag': 1382, 'dragon': 1383, 'drained': 1384, 'dramatically': 1385, 'dread': 1386, 'dreadful': 1387, 'dreadfulobviously': 1388, 'dream': 1389, 'dreamdisneyland': 1390, 'dreamingbad': 1391, 'dreams': 1392, 'dress': 1393, 'dressed': 1394, 'dressing': 1395, 'drink': 1396, 'drinkable': 1397, 'drinking': 1398, 'drinks': 1399, 'drive': 1400, 'driver': 1401, 'driving': 1402, 'drizzle': 1403, 'drizzly': 1404, 'drop': 1405, 'dropped': 1406, 'drops': 1407, 'dropwith': 1408, 'drove': 1409, 'dry': 1410, 'dual': 1411, 'dubious': 1412, 'duck': 1413, 'ducking': 1414, 'ducks': 1415, 'dude': 1416, 'due': 1417, 'dull': 1418, 'dumbo': 1419, 'dumbos': 1420, 'dump': 1421, 'durin': 1422, 'during': 1423, 'dvds': 1424, 'dw': 1425, 'dystrophy': 1426, 'e': 1427, 'each': 1428, 'ear': 1429, 'earl': 1430, 'earlier': 1431, 'earliest': 1432, 'early': 1433, 'earlyas': 1434, 'earlywaste': 1435, 'earned': 1436, 'ears': 1437, 'earth': 1438, 'earthso': 1439, 'ease': 1440, 'easier': 1441, 'easierthere': 1442, 'easiest': 1443, 'easily': 1444, 'east': 1445, 'easter': 1446, 'eastern': 1447, 'easy': 1448, 'eat': 1449, 'eaten': 1450, 'eater': 1451, 'eateries': 1452, 'eatif': 1453, 'eating': 1454, 'ecstatic': 1455, 'editions': 1456, 'eeyore': 1457, 'effect': 1458, 'effectsprice': 1459, 'efficient': 1460, 'efficiently': 1461, 'effort': 1462, 'eg': 1463, 'eggs': 1464, 'ehr': 1465, 'eiffel': 1466, 'eight': 1467, 'either': 1468, 'eitherthe': 1469, 'eitherthere': 1470, 'elderly': 1471, 'eldest': 1472, 'electric': 1473, 'electrical': 1474, 'electronically': 1475, 'elephant': 1476, 'elephantelephant': 1477, 'eleven': 1478, 'eliminate': 1479, 'eliminated': 1480, 'elsa': 1481, 'else': 1482, 'elses': 1483, 'elsewhere': 1484, 'elswhere': 1485, 'elysee': 1486, 'email': 1487, 'emails': 1488, 'embarrassing': 1489, 'embarrassment': 1490, 'emergency': 1491, 'emh': 1492, 'emotional': 1493, 'emotions': 1494, 'emphasis': 1495, 'empire': 1496, 'employee': 1497, 'employees': 1498, 'empty': 1499, 'enabled': 1500, 'enables': 1501, 'enchanted': 1502, 'enchanting': 1503, 'encounted': 1504, 'encounter': 1505, 'encountered': 1506, 'encourage': 1507, 'encourages': 1508, 'end': 1509, 'endafter': 1510, 'ended': 1511, 'ending': 1512, 'endless': 1513, 'endlesscould': 1514, 'ends': 1515, 'endthe': 1516, 'endure': 1517, 'energy': 1518, 'enforce': 1519, 'enforced': 1520, 'enforces': 1521, 'engage': 1522, 'england': 1523, 'english': 1524, 'englishman': 1525, 'englishtoy': 1526, 'enhance': 1527, 'enjoy': 1528, 'enjoyable': 1529, 'enjoyablethe': 1530, 'enjoyed': 1531, 'enjoyfound': 1532, 'enjoying': 1533, 'enjoyit': 1534, 'enjoyment': 1535, 'enormous': 1536, 'enough': 1537, 'enroll': 1538, 'ensure': 1539, 'enter': 1540, 'enterance': 1541, 'entered': 1542, 'entering': 1543, 'entertained': 1544, 'entertaining': 1545, 'entertainment': 1546, 'enthralled': 1547, 'enthusiasm': 1548, 'enthusiastic': 1549, 'enticing': 1550, 'entire': 1551, 'entirely': 1552, 'entitled': 1553, 'entrance': 1554, 'entrances': 1555, 'entrancewe': 1556, 'entries': 1557, 'entry': 1558, 'entryone': 1559, 'environment': 1560, 'epic': 1561, 'eprfect': 1562, 'equally': 1563, 'equipped': 1564, 'error': 1565, 'escalator': 1566, 'escape': 1567, 'escort': 1568, 'escorting': 1569, 'esp': 1570, 'especially': 1571, 'especiallywe': 1572, 'espsnow': 1573, 'essential': 1574, 'essentially': 1575, 'establishment': 1576, 'estimate': 1577, 'etc': 1578, 'etcwhat': 1579, 'ethos': 1580, 'etickets': 1581, 'etiquette': 1582, 'etiquite': 1583, 'ette': 1584, 'eur': 1585, 'euro': 1586, 'eurodisney': 1587, 'europe': 1588, 'europeans': 1589, 'euros': 1590, 'evacuate': 1591, 'eve': 1592, 'even': 1593, 'evening': 1594, 'evenings': 1595, 'event': 1596, 'events': 1597, 'eventually': 1598, 'evenwhen': 1599, 'ever': 1600, 'everest': 1601, 'evereything': 1602, 'every': 1603, 'everybody': 1604, 'everyday': 1605, 'everyone': 1606, 'everyonethe': 1607, 'everything': 1608, 'everythingmy': 1609, 'everytime': 1610, 'everywhere': 1611, 'everywhereall': 1612, 'everywhereeven': 1613, 'evidenttoilets': 1614, 'evolution': 1615, 'evrything': 1616, 'exact': 1617, 'exactly': 1618, 'example': 1619, 'examplei': 1620, 'examples': 1621, 'excelent': 1622, 'excellent': 1623, 'except': 1624, 'exception': 1625, 'exceptional': 1626, 'exceptionally': 1627, 'excess': 1628, 'excessive': 1629, 'exchange': 1630, 'excited': 1631, 'excitement': 1632, 'excitementwork': 1633, 'exciting': 1634, 'exclusively': 1635, 'excruciatingly': 1636, 'excursions': 1637, 'excuse': 1638, 'exhausted': 1639, 'exhausting': 1640, 'exhaustingwe': 1641, 'exhibits': 1642, 'existed': 1643, 'existent': 1644, 'exit': 1645, 'exited': 1646, 'exorbitant': 1647, 'expand': 1648, 'expanding': 1649, 'expansive': 1650, 'expect': 1651, 'expectation': 1652, 'expectations': 1653, 'expected': 1654, 'expectedwe': 1655, 'expecting': 1656, 'expects': 1657, 'expense': 1658, 'expenses': 1659, 'expensive': 1660, 'expensivecheck': 1661, 'expensivedefinitely': 1662, 'expensiveone': 1663, 'expensiveour': 1664, 'expensivetried': 1665, 'experiance': 1666, 'experience': 1667, 'experienceback': 1668, 'experienced': 1669, 'experiences': 1670, 'experiencethe': 1671, 'experiencewe': 1672, 'experiencing': 1673, 'experts': 1674, 'explain': 1675, 'explains': 1676, 'explanation': 1677, 'explore': 1678, 'explorers': 1679, 'exploring': 1680, 'exposing': 1681, 'express': 1682, 'exsagerated': 1683, 'exspensive': 1684, 'extensive': 1685, 'extortionate': 1686, 'extortionately': 1687, 'extra': 1688, 'extracting': 1689, 'extreme': 1690, 'extremely': 1691, 'exudes': 1692, 'eye': 1693, 'eyes': 1694, 'fab': 1695, 'fabulous': 1696, 'face': 1697, 'facehowever': 1698, 'facei': 1699, 'facelift': 1700, 'faces': 1701, 'facilities': 1702, 'facilitiesn': 1703, 'facility': 1704, 'fact': 1705, 'factor': 1706, 'factory': 1707, 'failed': 1708, 'fails': 1709, 'fair': 1710, 'fairies': 1711, 'fairly': 1712, 'fairy': 1713, 'fairytale': 1714, 'fajitas': 1715, 'fake': 1716, 'fall': 1717, 'falling': 1718, 'familes': 1719, 'familiar': 1720, 'families': 1721, 'family': 1722, 'familyi': 1723, 'familys': 1724, 'familythere': 1725, 'famous': 1726, 'fan': 1727, 'fanastic': 1728, 'fanatic': 1729, 'fangirls': 1730, 'fans': 1731, 'fantasmagorical': 1732, 'fantasmic': 1733, 'fantastic': 1734, 'fantasticall': 1735, 'fantastmic': 1736, 'fantasy': 1737, 'fantasyland': 1738, 'far': 1739, 'farewell': 1740, 'farm': 1741, 'farther': 1742, 'farthest': 1743, 'fascinating': 1744, 'fashion': 1745, 'fashioned': 1746, 'fast': 1747, 'faster': 1748, 'fastest': 1749, 'fastnegative': 1750, 'fastpass': 1751, 'fastpasses': 1752, 'fastpasslonger': 1753, 'fastpast': 1754, 'fatty': 1755, 'fault': 1756, 'faultedwe': 1757, 'faults': 1758, 'faultthe': 1759, 'fav': 1760, 'fave': 1761, 'faves': 1762, 'favor': 1763, 'favorite': 1764, 'favorites': 1765, 'favourite': 1766, 'favourites': 1767, 'favurite': 1768, 'fe': 1769, 'fealing': 1770, 'feature': 1771, 'features': 1772, 'featuring': 1773, 'feb': 1774, 'february': 1775, 'fed': 1776, 'fee': 1777, 'feed': 1778, 'feedback': 1779, 'feel': 1780, 'feeling': 1781, 'feelings': 1782, 'feels': 1783, 'fees': 1784, 'feet': 1785, 'fell': 1786, 'fellow': 1787, 'felt': 1788, 'fence': 1789, 'ferris': 1790, 'festival': 1791, 'festive': 1792, 'festivities': 1793, 'few': 1794, 'fewer': 1795, 'fi': 1796, 'fiancee': 1797, 'fibre': 1798, 'fidgeting': 1799, 'fifteen': 1800, 'fifth': 1801, 'fight': 1802, 'fighting': 1803, 'figure': 1804, 'figured': 1805, 'figures': 1806, 'figuring': 1807, 'fill': 1808, 'filled': 1809, 'filling': 1810, 'fills': 1811, 'film': 1812, 'films': 1813, 'filthy': 1814, 'filthywe': 1815, 'final': 1816, 'finally': 1817, 'find': 1818, 'finde': 1819, 'finding': 1820, 'fine': 1821, 'finger': 1822, 'finish': 1823, 'finished': 1824, 'finishing': 1825, 'finnicky': 1826, 'fire': 1827, 'firework': 1828, 'fireworks': 1829, 'fireworkssleeping': 1830, 'fireworksvisitors': 1831, 'fireworkswhich': 1832, 'first': 1833, 'firstly': 1834, 'firworks': 1835, 'fit': 1836, 'five': 1837, 'fix': 1838, 'fixed': 1839, 'fixing': 1840, 'fizzy': 1841, 'fl': 1842, 'fla': 1843, 'flags': 1844, 'flash': 1845, 'flashing': 1846, 'flavour': 1847, 'flew': 1848, 'flexible': 1849, 'flight': 1850, 'flights': 1851, 'float': 1852, 'floats': 1853, 'flooding': 1854, 'floor': 1855, 'floors': 1856, 'florid': 1857, 'florida': 1858, 'floridahowever': 1859, 'floridapositives': 1860, 'floridian': 1861, 'flow': 1862, 'fluent': 1863, 'flustered': 1864, 'fly': 1865, 'flying': 1866, 'flys': 1867, 'fo': 1868, 'focus': 1869, 'focussed': 1870, 'folk': 1871, 'folking': 1872, 'folks': 1873, 'follow': 1874, 'followed': 1875, 'follows': 1876, 'food': 1877, 'foodalso': 1878, 'foods': 1879, 'foodso': 1880, 'fool': 1881, 'foot': 1882, 'footprint': 1883, 'footsteps': 1884, 'for': 1885, 'forcast': 1886, 'force': 1887, 'forecast': 1888, 'foreigners': 1889, 'forest': 1890, 'forever': 1891, 'forget': 1892, 'forgetgo': 1893, 'forgot': 1894, 'forgotten': 1895, 'fork': 1896, 'forks': 1897, 'form': 1898, 'formation': 1899, 'formed': 1900, 'former': 1901, 'forour': 1902, 'forth': 1903, 'forties': 1904, 'fortress': 1905, 'fortunate': 1906, 'fortune': 1907, 'forward': 1908, 'foul': 1909, 'found': 1910, 'foundfood': 1911, 'fountain': 1912, 'fountains': 1913, 'four': 1914, 'fourth': 1915, 'fowards': 1916, 'fp': 1917, 'france': 1918, 'franchise': 1919, 'franctic': 1920, 'frankly': 1921, 'frantic': 1922, 'fraud': 1923, 'fraudulent': 1924, 'freak': 1925, 'freaked': 1926, 'freakin': 1927, 'freaking': 1928, 'free': 1929, 'freelot': 1930, 'freely': 1931, 'frence': 1932, 'french': 1933, 'frequent': 1934, 'frequently': 1935, 'fresh': 1936, 'fri': 1937, 'friction': 1938, 'friday': 1939, 'fridays': 1940, 'fried': 1941, 'friend': 1942, 'friendlier': 1943, 'friendliness': 1944, 'friendly': 1945, 'friends': 1946, 'fries': 1947, 'frightened': 1948, 'frightening': 1949, 'fro': 1950, 'from': 1951, 'front': 1952, 'frontage': 1953, 'fronteerland': 1954, 'frontierland': 1955, 'frosting': 1956, 'frown': 1957, 'frozen': 1958, 'fruit': 1959, 'frustrated': 1960, 'frustrating': 1961, 'frustratingi': 1962, 'frustratingthe': 1963, 'fudge': 1964, 'fuirculate': 1965, 'fulfill': 1966, 'fulfilled': 1967, 'full': 1968, 'fully': 1969, 'fun': 1970, 'funnelled': 1971, 'funny': 1972, 'furious': 1973, 'furnished': 1974, 'further': 1975, 'furthermore': 1976, 'furthest': 1977, 'fussy': 1978, 'future': 1979, 'fyi': 1980, 'gallery': 1981, 'galleryand': 1982, 'galons': 1983, 'games': 1984, 'gang': 1985, 'gardens': 1986, 'gaston': 1987, 'gate': 1988, 'gates': 1989, 'gather': 1990, 'gauging': 1991, 'gave': 1992, 'gay': 1993, 'geared': 1994, 'genaration': 1995, 'general': 1996, 'generally': 1997, 'generation': 1998, 'generations': 1999, 'genie': 2000, 'genius': 2001, 'gentle': 2002, 'gently': 2003, 'genuinedont': 2004, 'germans': 2005, 'get': 2006, 'getaway': 2007, 'getcertainly': 2008, 'gets': 2009, 'getting': 2010, 'ghost': 2011, 'giant': 2012, 'giddiness': 2013, 'giddy': 2014, 'gift': 2015, 'giggles': 2016, 'girl': 2017, 'girlfriend': 2018, 'girlfriends': 2019, 'girls': 2020, 'give': 2021, 'givehaving': 2022, 'given': 2023, 'giveng': 2024, 'giventom': 2025, 'gives': 2026, 'giving': 2027, 'glad': 2028, 'gladly': 2029, 'glands': 2030, 'glare': 2031, 'glass': 2032, 'glasses': 2033, 'glimpse': 2034, 'glitches': 2035, 'glued': 2036, 'gluten': 2037, 'go': 2038, 'god': 2039, 'goer': 2040, 'goers': 2041, 'goes': 2042, 'goinf': 2043, 'going': 2044, 'goinggrab': 2045, 'golden': 2046, 'gomickeys': 2047, 'gomorrah': 2048, 'gone': 2049, 'gonethe': 2050, 'gonna': 2051, 'goo': 2052, 'good': 2053, 'goodie': 2054, 'goodmystic': 2055, 'goodness': 2056, 'goods': 2057, 'goodthe': 2058, 'goofy': 2059, 'google': 2060, 'goosebumpsmaybe': 2061, 'got': 2062, 'goth': 2063, 'gotipsfood': 2064, 'gotta': 2065, 'gotten': 2066, 'gps': 2067, 'grab': 2068, 'grabbed': 2069, 'grace': 2070, 'grand': 2071, 'grandchildren': 2072, 'granddaughter': 2073, 'granddaughters': 2074, 'grandkids': 2075, 'grandparents': 2076, 'grandson': 2077, 'granted': 2078, 'graphics': 2079, 'grasses': 2080, 'grateful': 2081, 'graze': 2082, 'great': 2083, 'greater': 2084, 'greatest': 2085, 'greatly': 2086, 'greatthere': 2087, 'green': 2088, 'greet': 2089, 'greeting': 2090, 'greetings': 2091, 'greets': 2092, 'grew': 2093, 'grey': 2094, 'grill': 2095, 'grim': 2096, 'gripe': 2097, 'grizzly': 2098, 'grocery': 2099, 'gross': 2100, 'grotto': 2101, 'ground': 2102, 'grounds': 2103, 'group': 2104, 'groups': 2105, 'growing': 2106, 'grown': 2107, 'gruch': 2108, 'grumble': 2109, 'grumpy': 2110, 'guacamole': 2111, 'guarantee': 2112, 'guards': 2113, 'guess': 2114, 'guessing': 2115, 'guest': 2116, 'guestit': 2117, 'guests': 2118, 'guide': 2119, 'guidebook': 2120, 'guidemap': 2121, 'guides': 2122, 'gulch': 2123, 'gum': 2124, 'guy': 2125, 'guys': 2126, 'had': 2127, 'hadbut': 2128, 'hadnt': 2129, 'haggard': 2130, 'haha': 2131, 'hair': 2132, 'halal': 2133, 'half': 2134, 'halfterm': 2135, 'hall': 2136, 'halloween': 2137, 'hand': 2138, 'handed': 2139, 'handful': 2140, 'handfulspace': 2141, 'handicap': 2142, 'handicapped': 2143, 'handle': 2144, 'handling': 2145, 'hands': 2146, 'handt': 2147, 'handy': 2148, 'hang': 2149, 'happen': 2150, 'happened': 2151, 'happening': 2152, 'happens': 2153, 'happiest': 2154, 'happily': 2155, 'happiness': 2156, 'happinessonce': 2157, 'happy': 2158, 'happyi': 2159, 'happywatched': 2160, 'hard': 2161, 'hardcore': 2162, 'harder': 2163, 'hardly': 2164, 'haribo': 2165, 'harm': 2166, 'has': 2167, 'hassle': 2168, 'hassles': 2169, 'hat': 2170, 'hate': 2171, 'hated': 2172, 'hats': 2173, 'hatter': 2174, 'haunted': 2175, 'have': 2176, 'haven': 2177, 'havent': 2178, 'havevisited': 2179, 'having': 2180, 'he': 2181, 'head': 2182, 'headband': 2183, 'headed': 2184, 'heading': 2185, 'headthe': 2186, 'health': 2187, 'healthy': 2188, 'heaps': 2189, 'hear': 2190, 'heard': 2191, 'hearing': 2192, 'hears': 2193, 'heart': 2194, 'heat': 2195, 'heaven': 2196, 'heavily': 2197, 'heaving': 2198, 'heavy': 2199, 'hed': 2200, 'heels': 2201, 'height': 2202, 'held': 2203, 'hell': 2204, 'hello': 2205, 'help': 2206, 'helped': 2207, 'helpful': 2208, 'helpfulif': 2209, 'helpfull': 2210, 'helpfulness': 2211, 'helpfulthe': 2212, 'helping': 2213, 'helps': 2214, 'hence': 2215, 'her': 2216, 'herd': 2217, 'here': 2218, 'heres': 2219, 'herethere': 2220, 'heroes': 2221, 'hes': 2222, 'hesitant': 2223, 'hey': 2224, 'hidden': 2225, 'hide': 2226, 'high': 2227, 'higher': 2228, 'highest': 2229, 'highlight': 2230, 'highlights': 2231, 'highly': 2232, 'hightlights': 2233, 'hiked': 2234, 'hilarious': 2235, 'hill': 2236, 'him': 2237, 'himself': 2238, 'hint': 2239, 'hints': 2240, 'hire': 2241, 'hiring': 2242, 'his': 2243, 'historic': 2244, 'historical': 2245, 'history': 2246, 'hit': 2247, 'hk': 2248, 'hkd': 2249, 'hkit': 2250, 'ho': 2251, 'hojos': 2252, 'hold': 2253, 'holder': 2254, 'holders': 2255, 'holds': 2256, 'holdups': 2257, 'holes': 2258, 'holiday': 2259, 'holidayand': 2260, 'holidayi': 2261, 'holidaying': 2262, 'holidays': 2263, 'holidaythe': 2264, 'holloween': 2265, 'hollywood': 2266, 'home': 2267, 'homebe': 2268, 'homeworkon': 2269, 'honest': 2270, 'honestly': 2271, 'honeymoon': 2272, 'hong': 2273, 'hongkong': 2274, 'hook': 2275, 'hoot': 2276, 'hop': 2277, 'hope': 2278, 'hoped': 2279, 'hopefully': 2280, 'hopes': 2281, 'hoping': 2282, 'hopped': 2283, 'hopper': 2284, 'horrendous': 2285, 'horrible': 2286, 'horriblemr': 2287, 'horrified': 2288, 'horror': 2289, 'horse': 2290, 'hosts': 2291, 'hot': 2292, 'hotdog': 2293, 'hotdogs': 2294, 'hotel': 2295, 'hotelat': 2296, 'hotelhere': 2297, 'hotels': 2298, 'hotle': 2299, 'hotter': 2300, 'hottest': 2301, 'hounded': 2302, 'hour': 2303, 'hours': 2304, 'house': 2305, 'houseits': 2306, 'houses': 2307, 'housing': 2308, 'how': 2309, 'however': 2310, 'howeverin': 2311, 'hr': 2312, 'hrs': 2313, 'hrsif': 2314, 'http': 2315, 'huge': 2316, 'hugs': 2317, 'human': 2318, 'humanitarianism': 2319, 'humans': 2320, 'humdrum': 2321, 'humid': 2322, 'humidity': 2323, 'hundred': 2324, 'hundreds': 2325, 'hunger': 2326, 'hunt': 2327, 'hurry': 2328, 'hurts': 2329, 'husband': 2330, 'husbands': 2331, 'hydrated': 2332, 'hydratedwe': 2333, 'hype': 2334, 'hyper': 2335, 'hyperspace': 2336, 'i': 2337, 'ice': 2338, 'icing': 2339, 'iconic': 2340, 'iconya': 2341, 'id': 2342, 'idea': 2343, 'ideally': 2344, 'identify': 2345, 'ie': 2346, 'if': 2347, 'ignorant': 2348, 'ignore': 2349, 'ignored': 2350, 'ill': 2351, 'illumination': 2352, 'illuminations': 2353, 'illusion': 2354, 'im': 2355, 'images': 2356, 'imaginary': 2357, 'imagination': 2358, 'imaginationthey': 2359, 'imaginative': 2360, 'imagine': 2361, 'imagined': 2362, 'imagineers': 2363, 'imaginei': 2364, 'imitation': 2365, 'immaculate': 2366, 'immediately': 2367, 'immensely': 2368, 'impact': 2369, 'impeccable': 2370, 'implemented': 2371, 'important': 2372, 'importantly': 2373, 'impossibe': 2374, 'impossible': 2375, 'impress': 2376, 'impressed': 2377, 'impression': 2378, 'impressive': 2379, 'improved': 2380, 'improvement': 2381, 'improvements': 2382, 'improving': 2383, 'in': 2384, 'inability': 2385, 'inaccurate': 2386, 'inbetween': 2387, 'incidentally': 2388, 'inclement': 2389, 'include': 2390, 'included': 2391, 'includedbig': 2392, 'includes': 2393, 'including': 2394, 'inclusive': 2395, 'incomparable': 2396, 'inconvinience': 2397, 'incorrect': 2398, 'increased': 2399, 'incredible': 2400, 'incredibly': 2401, 'indeed': 2402, 'indefinitely': 2403, 'independence': 2404, 'india': 2405, 'indian': 2406, 'indiana': 2407, 'indiansfood': 2408, 'indicate': 2409, 'indicated': 2410, 'indifferent': 2411, 'individual': 2412, 'individuals': 2413, 'indoor': 2414, 'industry': 2415, 'inevitable': 2416, 'inexpensive': 2417, 'infact': 2418, 'inflated': 2419, 'influx': 2420, 'inform': 2421, 'information': 2422, 'informative': 2423, 'infront': 2424, 'ingredients': 2425, 'inhale': 2426, 'inhaled': 2427, 'injection': 2428, 'inline': 2429, 'inn': 2430, 'inner': 2431, 'innocent': 2432, 'innovations': 2433, 'innovator': 2434, 'innoventions': 2435, 'insane': 2436, 'insanely': 2437, 'inside': 2438, 'insisted': 2439, 'install': 2440, 'installations': 2441, 'installed': 2442, 'instance': 2443, 'instantlyfirst': 2444, 'instead': 2445, 'insteadwaited': 2446, 'instructions': 2447, 'insulated': 2448, 'insulting': 2449, 'insurance': 2450, 'insure': 2451, 'intend': 2452, 'intended': 2453, 'intense': 2454, 'intensely': 2455, 'inter': 2456, 'interact': 2457, 'interaction': 2458, 'interactive': 2459, 'interest': 2460, 'interested': 2461, 'interesting': 2462, 'interestingwe': 2463, 'international': 2464, 'internet': 2465, 'internetusually': 2466, 'intertesting': 2467, 'into': 2468, 'intosecond': 2469, 'introduce': 2470, 'introductory': 2471, 'introverti': 2472, 'invading': 2473, 'invalid': 2474, 'invest': 2475, 'invested': 2476, 'investment': 2477, 'invited': 2478, 'involved': 2479, 'involves': 2480, 'ion': 2481, 'iphone': 2482, 'iron': 2483, 'ironman': 2484, 'irritating': 2485, 'is': 2486, 'isango': 2487, 'isdo': 2488, 'isif': 2489, 'island': 2490, 'isle': 2491, 'isn': 2492, 'isnt': 2493, 'issue': 2494, 'issued': 2495, 'issues': 2496, 'issuesoh': 2497, 'it': 2498, 'italians': 2499, 'itand': 2500, 'iteither': 2501, 'items': 2502, 'iteverywhere': 2503, 'iti': 2504, 'itinerary': 2505, 'itit': 2506, 'itits': 2507, 'itll': 2508, 'itno': 2509, 'itone': 2510, 'itquite': 2511, 'its': 2512, 'itseemed': 2513, 'itself': 2514, 'itthe': 2515, 'itthis': 2516, 'itwe': 2517, 'ive': 2518, 'jack': 2519, 'jacket': 2520, 'jackythe': 2521, 'jaded': 2522, 'jam': 2523, 'jan': 2524, 'january': 2525, 'japan': 2526, 'japanese': 2527, 'jedi': 2528, 'jerking': 2529, 'jerrys': 2530, 'jessie': 2531, 'jingle': 2532, 'jitters': 2533, 'job': 2534, 'jobs': 2535, 'jobsworth': 2536, 'join': 2537, 'joined': 2538, 'joining': 2539, 'joint': 2540, 'joke': 2541, 'jones': 2542, 'jos': 2543, 'journey': 2544, 'joy': 2545, 'joyful': 2546, 'juan': 2547, 'jubilant': 2548, 'juggernaut': 2549, 'juice': 2550, 'july': 2551, 'julyso': 2552, 'jump': 2553, 'jumped': 2554, 'jumpers': 2555, 'jumpersthe': 2556, 'jumping': 2557, 'jumps': 2558, 'june': 2559, 'jungle': 2560, 'junk': 2561, 'junkie': 2562, 'junkies': 2563, 'junky': 2564, 'just': 2565, 'justify': 2566, 'k': 2567, 'kaos': 2568, 'keep': 2569, 'keeping': 2570, 'keeps': 2571, 'keepsake': 2572, 'kept': 2573, 'key': 2574, 'kick': 2575, 'kicked': 2576, 'kid': 2577, 'kidalso': 2578, 'kiddies': 2579, 'kiddingin': 2580, 'kiddos': 2581, 'kids': 2582, 'kidsdont': 2583, 'kidsfirst': 2584, 'kidsi': 2585, 'kidslots': 2586, 'kidsmany': 2587, 'kidsmy': 2588, 'kidsthere': 2589, 'kidswe': 2590, 'kill': 2591, 'killer': 2592, 'killing': 2593, 'kind': 2594, 'kinda': 2595, 'kinde': 2596, 'kindly': 2597, 'kinds': 2598, 'king': 2599, 'kingdom': 2600, 'kingdoms': 2601, 'kiosk': 2602, 'kiss': 2603, 'knackard': 2604, 'knees': 2605, 'knew': 2606, 'knives': 2607, 'knocked': 2608, 'knocks': 2609, 'knotched': 2610, 'knotts': 2611, 'know': 2612, 'knowbut': 2613, 'knowing': 2614, 'knowledge': 2615, 'known': 2616, 'knows': 2617, 'kok': 2618, 'kong': 2619, 'kowloon': 2620, 'krispy': 2621, 'kurt': 2622, 'la': 2623, 'labor': 2624, 'labour': 2625, 'labyrinth': 2626, 'lack': 2627, 'lacked': 2628, 'lacking': 2629, 'lacks': 2630, 'lad': 2631, 'lady': 2632, 'lagoon': 2633, 'lakea': 2634, 'lance': 2635, 'land': 2636, 'landingbuzz': 2637, 'lands': 2638, 'landscaped': 2639, 'landsthere': 2640, 'landtickets': 2641, 'lane': 2642, 'lanes': 2643, 'lanethe': 2644, 'language': 2645, 'languages': 2646, 'lantau': 2647, 'large': 2648, 'larger': 2649, 'las': 2650, 'laser': 2651, 'lasers': 2652, 'last': 2653, 'lasted': 2654, 'lastly': 2655, 'lasts': 2656, 'late': 2657, 'later': 2658, 'lateralso': 2659, 'latest': 2660, 'latter': 2661, 'laughed': 2662, 'laughing': 2663, 'laughterwe': 2664, 'layout': 2665, 'layover': 2666, 'lead': 2667, 'leading': 2668, 'league': 2669, 'learn': 2670, 'learned': 2671, 'learnedit': 2672, 'learnt': 2673, 'leary': 2674, 'least': 2675, 'leave': 2676, 'leavei': 2677, 'leaves': 2678, 'leaving': 2679, 'left': 2680, 'leftdisney': 2681, 'leg': 2682, 'lego': 2683, 'legoland': 2684, 'legs': 2685, 'lemonade': 2686, 'length': 2687, 'lengths': 2688, 'lengthy': 2689, 'less': 2690, 'lesson': 2691, 'lest': 2692, 'let': 2693, 'lets': 2694, 'letting': 2695, 'level': 2696, 'levels': 2697, 'lf': 2698, 'liberate': 2699, 'license': 2700, 'life': 2701, 'lifetime': 2702, 'lift': 2703, 'light': 2704, 'lighted': 2705, 'lighter': 2706, 'lighteyear': 2707, 'lighting': 2708, 'lightly': 2709, 'lights': 2710, 'lightshow': 2711, 'lightyear': 2712, 'lightyears': 2713, 'lighyyear': 2714, 'like': 2715, 'liked': 2716, 'likely': 2717, 'likes': 2718, 'likewise': 2719, 'lil': 2720, 'lilo': 2721, 'limitations': 2722, 'limited': 2723, 'limiting': 2724, 'limits': 2725, 'lincoln': 2726, 'line': 2727, 'lined': 2728, 'lines': 2729, 'linesi': 2730, 'linethe': 2731, 'lingual': 2732, 'lining': 2733, 'link': 2734, 'lion': 2735, 'list': 2736, 'listed': 2737, 'listen': 2738, 'listened': 2739, 'lit': 2740, 'lite': 2741, 'literally': 2742, 'litter': 2743, 'littke': 2744, 'little': 2745, 'live': 2746, 'lived': 2747, 'lives': 2748, 'living': 2749, 'll': 2750, 'load': 2751, 'loads': 2752, 'loathe': 2753, 'local': 2754, 'locals': 2755, 'locate': 2756, 'located': 2757, 'location': 2758, 'locations': 2759, 'locker': 2760, 'lockers': 2761, 'lodge': 2762, 'logo': 2763, 'lol': 2764, 'lolhowever': 2765, 'lolly': 2766, 'long': 2767, 'longed': 2768, 'longer': 2769, 'longest': 2770, 'longgg': 2771, 'longmy': 2772, 'longthe': 2773, 'longyes': 2774, 'lonq': 2775, 'look': 2776, 'looked': 2777, 'looking': 2778, 'looks': 2779, 'los': 2780, 'lose': 2781, 'loses': 2782, 'lost': 2783, 'lostso': 2784, 'lot': 2785, 'lots': 2786, 'lou': 2787, 'loud': 2788, 'loudly': 2789, 'love': 2790, 'loved': 2791, 'lovely': 2792, 'loves': 2793, 'loving': 2794, 'lovingly': 2795, 'low': 2796, 'lowdisneyland': 2797, 'lower': 2798, 'lowest': 2799, 'luck': 2800, 'luckily': 2801, 'luckliy': 2802, 'lucky': 2803, 'luggage': 2804, 'luigis': 2805, 'lunch': 2806, 'lunchkids': 2807, 'lunchtime': 2808, 'luuuuuhved': 2809, 'luv': 2810, 'luxury': 2811, 'lying': 2812, 'm': 2813, 'machine': 2814, 'machines': 2815, 'mad': 2816, 'maddening': 2817, 'maddness': 2818, 'made': 2819, 'madness': 2820, 'mafia': 2821, 'magic': 2822, 'magical': 2823, 'magicalthe': 2824, 'magician': 2825, 'magics': 2826, 'magicthere': 2827, 'magnificent': 2828, 'mailing': 2829, 'maimise': 2830, 'main': 2831, 'mainland': 2832, 'mainly': 2833, 'mainstreet': 2834, 'maintain': 2835, 'maintained': 2836, 'maintaining': 2837, 'maintenance': 2838, 'major': 2839, 'majority': 2840, 'make': 2841, 'makeover': 2842, 'makes': 2843, 'makeup': 2844, 'making': 2845, 'malaysian': 2846, 'malaysians': 2847, 'male': 2848, 'malfunction': 2849, 'malfunctioned': 2850, 'mam': 2851, 'man': 2852, 'manage': 2853, 'manageable': 2854, 'managed': 2855, 'management': 2856, 'manager': 2857, 'manages': 2858, 'managing': 2859, 'managment': 2860, 'manchester': 2861, 'mandarin': 2862, 'mandatory': 2863, 'manhandle': 2864, 'manic': 2865, 'manila': 2866, 'manner': 2867, 'manners': 2868, 'mannersthe': 2869, 'manning': 2870, 'manor': 2871, 'mansio': 2872, 'mansion': 2873, 'mantience': 2874, 'manual': 2875, 'many': 2876, 'map': 2877, 'maps': 2878, 'marathon': 2879, 'march': 2880, 'marched': 2881, 'marginally': 2882, 'mark': 2883, 'marked': 2884, 'market': 2885, 'marne': 2886, 'marriott': 2887, 'martinisydney': 2888, 'marvel': 2889, 'marvellous': 2890, 'marvelous': 2891, 'mary': 2892, 'mass': 2893, 'massive': 2894, 'massively': 2895, 'match': 2896, 'matches': 2897, 'matter': 2898, 'matterhorn': 2899, 'max': 2900, 'maximum': 2901, 'maxpass': 2902, 'may': 2903, 'maybe': 2904, 'maze': 2905, 'mcdonalds': 2906, 'me': 2907, 'meal': 2908, 'meals': 2909, 'mean': 2910, 'meaningful': 2911, 'means': 2912, 'meant': 2913, 'meanwhile': 2914, 'measure': 2915, 'measures': 2916, 'mechanical': 2917, 'medical': 2918, 'mediocre': 2919, 'medium': 2920, 'meet': 2921, 'meeting': 2922, 'meets': 2923, 'melbourne': 2924, 'melted': 2925, 'member': 2926, 'members': 2927, 'memorabillia': 2928, 'memorable': 2929, 'memories': 2930, 'memory': 2931, 'mentality': 2932, 'mention': 2933, 'mentioned': 2934, 'menu': 2935, 'menus': 2936, 'merchandise': 2937, 'merchandiseonly': 2938, 'merely': 2939, 'merry': 2940, 'mesa': 2941, 'messages': 2942, 'messes': 2943, 'met': 2944, 'metal': 2945, 'meteors': 2946, 'meters': 2947, 'methere': 2948, 'metro': 2949, 'metrogot': 2950, 'mexican': 2951, 'michigan': 2952, 'mickey': 2953, 'mickeyprices': 2954, 'mickeys': 2955, 'micky': 2956, 'mickyminnie': 2957, 'microwave': 2958, 'mid': 2959, 'middle': 2960, 'midnight': 2961, 'midweek': 2962, 'midwestern': 2963, 'might': 2964, 'mildly': 2965, 'mile': 2966, 'miles': 2967, 'military': 2968, 'million': 2969, 'millions': 2970, 'min': 2971, 'mind': 2972, 'minders': 2973, 'mindi': 2974, 'mindset': 2975, 'mine': 2976, 'mingling': 2977, 'mini': 2978, 'miniature': 2979, 'minimal': 2980, 'minimum': 2981, 'minnie': 2982, 'minregarding': 2983, 'mins': 2984, 'minuet': 2985, 'minuets': 2986, 'minute': 2987, 'minutes': 2988, 'mirror': 2989, 'miscommunication': 2990, 'miserable': 2991, 'miss': 2992, 'missed': 2993, 'missedthere': 2994, 'missing': 2995, 'mission': 2996, 'mistake': 2997, 'mix': 2998, 'mixed': 2999, 'mlk': 3000, 'moan': 3001, 'moana': 3002, 'moaned': 3003, 'mobile': 3004, 'mobility': 3005, 'mode': 3006, 'model': 3007, 'moderate': 3008, 'modern': 3009, 'mom': 3010, 'moment': 3011, 'moments': 3012, 'momentthe': 3013, 'mon': 3014, 'monday': 3015, 'money': 3016, 'moneys': 3017, 'mong': 3018, 'monitored': 3019, 'monitors': 3020, 'monorail': 3021, 'monster': 3022, 'month': 3023, 'months': 3024, 'mood': 3025, 'moping': 3026, 'more': 3027, 'moremy': 3028, 'moreto': 3029, 'mornig': 3030, 'morning': 3031, 'morninghowever': 3032, 'mornings': 3033, 'moroccan': 3034, 'most': 3035, 'mostly': 3036, 'mosts': 3037, 'motel': 3038, 'mother': 3039, 'motif': 3040, 'motion': 3041, 'motto': 3042, 'mountain': 3043, 'mountaingreat': 3044, 'mountainindian': 3045, 'mouse': 3046, 'mouth': 3047, 'move': 3048, 'moved': 3049, 'movement': 3050, 'moves': 3051, 'moveso': 3052, 'movie': 3053, 'movies': 3054, 'moving': 3055, 'mr': 3056, 'mt': 3057, 'mtn': 3058, 'mtr': 3059, 'much': 3060, 'muchthe': 3061, 'muchwas': 3062, 'mucky': 3063, 'mugs': 3064, 'mulholland': 3065, 'multi': 3066, 'multiple': 3067, 'mum': 3068, 'muppet': 3069, 'muscular': 3070, 'museums': 3071, 'music': 3072, 'musical': 3073, 'musicbuss': 3074, 'muslims': 3075, 'must': 3076, 'mustmy': 3077, 'mwrs': 3078, 'my': 3079, 'myself': 3080, 'mystic': 3081, 'mystique': 3082, 'n': 3083, 'name': 3084, 'nameonly': 3085, 'names': 3086, 'nap': 3087, 'narration': 3088, 'narrative': 3089, 'narrow': 3090, 'nastier': 3091, 'nasty': 3092, 'nationals': 3093, 'nationfrom': 3094, 'naturally': 3095, 'nature': 3096, 'navigate': 3097, 'navigated': 3098, 'navigating': 3099, 'near': 3100, 'nearby': 3101, 'nearer': 3102, 'nearest': 3103, 'nearly': 3104, 'neat': 3105, 'necessary': 3106, 'neck': 3107, 'need': 3108, 'needed': 3109, 'needs': 3110, 'negative': 3111, 'negatives': 3112, 'neighbouring': 3113, 'nemo': 3114, 'nephew': 3115, 'nerds': 3116, 'never': 3117, 'nevertheless': 3118, 'new': 3119, 'newer': 3120, 'newest': 3121, 'newly': 3122, 'newport': 3123, 'next': 3124, 'ngong': 3125, 'nice': 3126, 'nicebe': 3127, 'nicely': 3128, 'nicer': 3129, 'niece': 3130, 'nieces': 3131, 'niggles': 3132, 'nigh': 3133, 'night': 3134, 'nighteverything': 3135, 'nightly': 3136, 'nightmare': 3137, 'nights': 3138, 'nighttime': 3139, 'nightwhen': 3140, 'nine': 3141, 'nique': 3142, 'no': 3143, 'nobody': 3144, 'nombers': 3145, 'non': 3146, 'none': 3147, 'nonethe': 3148, 'nonetheless': 3149, 'noon': 3150, 'nor': 3151, 'norm': 3152, 'normal': 3153, 'normality': 3154, 'normally': 3155, 'nose': 3156, 'nostalgic': 3157, 'not': 3158, 'notch': 3159, 'note': 3160, 'notealso': 3161, 'noted': 3162, 'nothing': 3163, 'notice': 3164, 'noticeable': 3165, 'noticed': 3166, 'notices': 3167, 'notified': 3168, 'nov': 3169, 'november': 3170, 'now': 3171, 'nowdays': 3172, 'nowhere': 3173, 'nugget': 3174, 'nuggets': 3175, 'nuisancelife': 3176, 'number': 3177, 'numbers': 3178, 'numerous': 3179, 'nurse': 3180, 'nut': 3181, 'nuts': 3182, 'nz': 3183, 'o': 3184, 'obeyed': 3185, 'obliterated': 3186, 'obscene': 3187, 'observation': 3188, 'obsessed': 3189, 'obvious': 3190, 'obviously': 3191, 'obvs': 3192, 'oc': 3193, 'occasion': 3194, 'occasional': 3195, 'occasionallyparking': 3196, 'occasions': 3197, 'ocean': 3198, 'oclock': 3199, 'ocotber': 3200, 'oct': 3201, 'october': 3202, 'odd': 3203, 'of': 3204, 'ofcourse': 3205, 'off': 3206, 'offensive': 3207, 'offer': 3208, 'offered': 3209, 'offering': 3210, 'offers': 3211, 'office': 3212, 'officers': 3213, 'official': 3214, 'offsite': 3215, 'often': 3216, 'ofwe': 3217, 'oh': 3218, 'ok': 3219, 'okay': 3220, 'okoverall': 3221, 'okso': 3222, 'old': 3223, 'older': 3224, 'olderthe': 3225, 'olds': 3226, 'oldwe': 3227, 'on': 3228, 'once': 3229, 'one': 3230, 'oneas': 3231, 'onefirst': 3232, 'onefor': 3233, 'onei': 3234, 'ones': 3235, 'onewe': 3236, 'onfood': 3237, 'online': 3238, 'onlinefor': 3239, 'only': 3240, 'onsite': 3241, 'onto': 3242, 'onwards': 3243, 'onwe': 3244, 'oops': 3245, 'open': 3246, 'opened': 3247, 'opening': 3248, 'openly': 3249, 'opens': 3250, 'opensget': 3251, 'operate': 3252, 'operating': 3253, 'operation': 3254, 'operational': 3255, 'operative': 3256, 'opinion': 3257, 'opotunity': 3258, 'opportunities': 3259, 'opportunity': 3260, 'opposite': 3261, 'opps': 3262, 'ops': 3263, 'opted': 3264, 'optimistic': 3265, 'option': 3266, 'options': 3267, 'or': 3268, 'orbitron': 3269, 'orchestra': 3270, 'order': 3271, 'ordered': 3272, 'orders': 3273, 'ordinary': 3274, 'organisation': 3275, 'organised': 3276, 'organized': 3277, 'orientated': 3278, 'oriented': 3279, 'original': 3280, 'originally': 3281, 'originals': 3282, 'orlando': 3283, 'orleans': 3284, 'other': 3285, 'others': 3286, 'othersclearly': 3287, 'otherwise': 3288, 'ouch': 3289, 'our': 3290, 'ours': 3291, 'ourselves': 3292, 'out': 3293, 'outdated': 3294, 'outdoor': 3295, 'outdoors': 3296, 'outer': 3297, 'outfits': 3298, 'outi': 3299, 'outlet': 3300, 'outlets': 3301, 'outnumbered': 3302, 'outrageous': 3303, 'outrageously': 3304, 'outs': 3305, 'outside': 3306, 'outstanding': 3307, 'outthe': 3308, 'oven': 3309, 'over': 3310, 'overall': 3311, 'overcast': 3312, 'overcrowded': 3313, 'overcrowding': 3314, 'overgrown': 3315, 'overlooking': 3316, 'overlooks': 3317, 'overly': 3318, 'overpriced': 3319, 'overqueues': 3320, 'overseas': 3321, 'overshadowed': 3322, 'oversized': 3323, 'overtime': 3324, 'overwhelmed': 3325, 'overwhelming': 3326, 'owe': 3327, 'own': 3328, 'p': 3329, 'pace': 3330, 'paced': 3331, 'paces': 3332, 'pacing': 3333, 'pack': 3334, 'package': 3335, 'packages': 3336, 'packed': 3337, 'packet': 3338, 'packets': 3339, 'page': 3340, 'paid': 3341, 'pain': 3342, 'paint': 3343, 'painted': 3344, 'painting': 3345, 'paintthe': 3346, 'pale': 3347, 'palette': 3348, 'pals': 3349, 'pan': 3350, 'panels': 3351, 'pans': 3352, 'panthe': 3353, 'paper': 3354, 'parachute': 3355, 'parade': 3356, 'paradeits': 3357, 'paradepleased': 3358, 'parades': 3359, 'paradesdidnt': 3360, 'paradise': 3361, 'parc': 3362, 'parcel': 3363, 'parent': 3364, 'parents': 3365, 'paris': 3366, 'parisand': 3367, 'parisi': 3368, 'parisparis': 3369, 'parissome': 3370, 'park': 3371, 'parkallow': 3372, 'parkbut': 3373, 'parkcap': 3374, 'parked': 3375, 'parketcfor': 3376, 'parkfood': 3377, 'parkhopper': 3378, 'parki': 3379, 'parking': 3380, 'parkparades': 3381, 'parks': 3382, 'parksbut': 3383, 'parksfor': 3384, 'parkside': 3385, 'parkslots': 3386, 'parksthe': 3387, 'parkswe': 3388, 'parkthe': 3389, 'parkthere': 3390, 'parkwe': 3391, 'part': 3392, 'participate': 3393, 'particular': 3394, 'particularly': 3395, 'parties': 3396, 'partner': 3397, 'partners': 3398, 'parts': 3399, 'partspark': 3400, 'party': 3401, 'partyseriously': 3402, 'pass': 3403, 'passbut': 3404, 'passe': 3405, 'passed': 3406, 'passengers': 3407, 'passes': 3408, 'passesif': 3409, 'passholder': 3410, 'passholders': 3411, 'passing': 3412, 'passion': 3413, 'passwalt': 3414, 'passwe': 3415, 'passyes': 3416, 'past': 3417, 'pasta': 3418, 'pastry': 3419, 'patch': 3420, 'path': 3421, 'pathetic': 3422, 'paths': 3423, 'patience': 3424, 'patient': 3425, 'patiently': 3426, 'patrons': 3427, 'pause': 3428, 'pavement': 3429, 'pavilion': 3430, 'pax': 3431, 'pay': 3432, 'paying': 3433, 'pays': 3434, 'pch': 3435, 'peak': 3436, 'pedestrian': 3437, 'pedometer': 3438, 'pelasantly': 3439, 'penalized': 3440, 'peninusla': 3441, 'penny': 3442, 'people': 3443, 'peoplefrench': 3444, 'peoplehere': 3445, 'peoples': 3446, 'per': 3447, 'perelopacheno': 3448, 'perfect': 3449, 'perfectly': 3450, 'perform': 3451, 'performance': 3452, 'performed': 3453, 'performers': 3454, 'perhaps': 3455, 'peril': 3456, 'perimeter': 3457, 'period': 3458, 'periodically': 3459, 'permanent': 3460, 'permitted': 3461, 'person': 3462, 'personal': 3463, 'personalised': 3464, 'personalities': 3465, 'personally': 3466, 'persons': 3467, 'perspective': 3468, 'pervilion': 3469, 'pete': 3470, 'peter': 3471, 'phantom': 3472, 'phanton': 3473, 'phenomenal': 3474, 'philarmagicthe': 3475, 'philharmagic': 3476, 'philharmonic': 3477, 'phillips': 3478, 'phone': 3479, 'phones': 3480, 'photo': 3481, 'photographers': 3482, 'photographing': 3483, 'photographsthe': 3484, 'photopass': 3485, 'photos': 3486, 'physically': 3487, 'pic': 3488, 'pick': 3489, 'picked': 3490, 'pickie': 3491, 'picks': 3492, 'picnic': 3493, 'picnics': 3494, 'pics': 3495, 'pictorialone': 3496, 'picture': 3497, 'pictures': 3498, 'picturesque': 3499, 'pie': 3500, 'piece': 3501, 'pieces': 3502, 'pieto': 3503, 'piggy': 3504, 'pile': 3505, 'pilgramage': 3506, 'pin': 3507, 'pineapple': 3508, 'ping': 3509, 'pinocchio': 3510, 'pinochio': 3511, 'pins': 3512, 'pirate': 3513, 'pirates': 3514, 'pitch': 3515, 'pity': 3516, 'pizza': 3517, 'pizzas': 3518, 'pizzathis': 3519, 'place': 3520, 'placea': 3521, 'places': 3522, 'plague': 3523, 'plagued': 3524, 'plain': 3525, 'plan': 3526, 'plane': 3527, 'planet': 3528, 'planned': 3529, 'planning': 3530, 'plans': 3531, 'plastic': 3532, 'plates': 3533, 'play': 3534, 'played': 3535, 'playground': 3536, 'playing': 3537, 'plaza': 3538, 'pleasant': 3539, 'pleasantly': 3540, 'please': 3541, 'pleased': 3542, 'pleasing': 3543, 'plenty': 3544, 'plot': 3545, 'pls': 3546, 'plus': 3547, 'pluses': 3548, 'pluto': 3549, 'plutos': 3550, 'pm': 3551, 'pocahontas': 3552, 'pocket': 3553, 'point': 3554, 'pointed': 3555, 'points': 3556, 'pointthe': 3557, 'police': 3558, 'policy': 3559, 'polite': 3560, 'politically': 3561, 'pond': 3562, 'ponds': 3563, 'pooh': 3564, 'pool': 3565, 'pools': 3566, 'poor': 3567, 'pooyou': 3568, 'pop': 3569, 'popcorn': 3570, 'poppins': 3571, 'popular': 3572, 'population': 3573, 'portaventura': 3574, 'portions': 3575, 'posing': 3576, 'position': 3577, 'positioning': 3578, 'positive': 3579, 'possibilities': 3580, 'possibility': 3581, 'possible': 3582, 'possibleand': 3583, 'possiblei': 3584, 'possibly': 3585, 'post': 3586, 'posted': 3587, 'potty': 3588, 'pottys': 3589, 'pound': 3590, 'pounds': 3591, 'poured': 3592, 'ppl': 3593, 'practical': 3594, 'practically': 3595, 'pram': 3596, 'pre': 3597, 'precious': 3598, 'predict': 3599, 'prefer': 3600, 'preferably': 3601, 'preferred': 3602, 'prefers': 3603, 'premium': 3604, 'prepare': 3605, 'prepared': 3606, 'preparer': 3607, 'prepaying': 3608, 'preplanning': 3609, 'present': 3610, 'presented': 3611, 'president': 3612, 'press': 3613, 'prestige': 3614, 'presumably': 3615, 'preteen': 3616, 'pretty': 3617, 'prevents': 3618, 'previous': 3619, 'previously': 3620, 'price': 3621, 'priced': 3622, 'priceless': 3623, 'prices': 3624, 'pricesenjoy': 3625, 'priceshowever': 3626, 'pricesthere': 3627, 'pricey': 3628, 'pride': 3629, 'prime': 3630, 'princess': 3631, 'princesses': 3632, 'princessesdrawback': 3633, 'print': 3634, 'printed': 3635, 'prior': 3636, 'prioritize': 3637, 'priority': 3638, 'pririty': 3639, 'privilege': 3640, 'privlege': 3641, 'pro': 3642, 'proabably': 3643, 'probably': 3644, 'problem': 3645, 'problemgrab': 3646, 'problems': 3647, 'problemswe': 3648, 'probs': 3649, 'process': 3650, 'procession': 3651, 'produced': 3652, 'production': 3653, 'productions': 3654, 'products': 3655, 'professionalism': 3656, 'professionally': 3657, 'profit': 3658, 'profits': 3659, 'program': 3660, 'programmes': 3661, 'progressed': 3662, 'progressively': 3663, 'prohibit': 3664, 'prohibiting': 3665, 'prohibition': 3666, 'projected': 3667, 'projections': 3668, 'promenade': 3669, 'promise': 3670, 'promised': 3671, 'promo': 3672, 'prone': 3673, 'proof': 3674, 'proper': 3675, 'properly': 3676, 'properlyalso': 3677, 'properties': 3678, 'property': 3679, 'proposed': 3680, 'pros': 3681, 'prospect': 3682, 'protect': 3683, 'protecting': 3684, 'proud': 3685, 'proved': 3686, 'proves': 3687, 'provide': 3688, 'provided': 3689, 'provides': 3690, 'providing': 3691, 'provisions': 3692, 'proviso': 3693, 'prrice': 3694, 'pub': 3695, 'public': 3696, 'publicbe': 3697, 'pudding': 3698, 'pull': 3699, 'pump': 3700, 'pumped': 3701, 'punished': 3702, 'puppets': 3703, 'purchase': 3704, 'purchased': 3705, 'purchasing': 3706, 'pure': 3707, 'purists': 3708, 'purposely': 3709, 'purse': 3710, 'push': 3711, 'pushchair': 3712, 'pushchairs': 3713, 'pushed': 3714, 'pushing': 3715, 'pushy': 3716, 'put': 3717, 'putting': 3718, 'pyjamas': 3719, 'q': 3720, 'qeue': 3721, 'quaintness': 3722, 'qualify': 3723, 'quality': 3724, 'quarter': 3725, 'que': 3726, 'qued': 3727, 'queen': 3728, 'queensland': 3729, 'queing': 3730, 'quenching': 3731, 'ques': 3732, 'question': 3733, 'queue': 3734, 'queued': 3735, 'queueing': 3736, 'queuemoreover': 3737, 'queues': 3738, 'queuesi': 3739, 'queueswe': 3740, 'queueswhere': 3741, 'queuewe': 3742, 'queuing': 3743, 'quick': 3744, 'quicker': 3745, 'quickerstaying': 3746, 'quickly': 3747, 'quiet': 3748, 'quieter': 3749, 'quieteronly': 3750, 'quieti': 3751, 'quietwalked': 3752, 'quinta': 3753, 'quite': 3754, 'quoting': 3755, 'r': 3756, 'rabbit': 3757, 'race': 3758, 'racer': 3759, 'racers': 3760, 'racing': 3761, 'raddison': 3762, 'radiator': 3763, 'radio': 3764, 'raft': 3765, 'railroad': 3766, 'railway': 3767, 'rain': 3768, 'rained': 3769, 'rainforest': 3770, 'raining': 3771, 'raininghad': 3772, 'rains': 3773, 'rainy': 3774, 'raise': 3775, 'raised': 3776, 'ram': 3777, 'ran': 3778, 'ranch': 3779, 'randomly': 3780, 'range': 3781, 'rapunzel': 3782, 'rare': 3783, 'rarely': 3784, 'ratatouille': 3785, 'ratatui': 3786, 'rate': 3787, 'ratesopls': 3788, 'rather': 3789, 'rating': 3790, 'rays': 3791, 'rc': 3792, 're': 3793, 'reach': 3794, 'reached': 3795, 'reaching': 3796, 'reactions': 3797, 'read': 3798, 'reading': 3799, 'reads': 3800, 'ready': 3801, 'real': 3802, 'realise': 3803, 'realised': 3804, 'realistic': 3805, 'reality': 3806, 'realize': 3807, 'realized': 3808, 'really': 3809, 'reared': 3810, 'reason': 3811, 'reasonable': 3812, 'reasonableall': 3813, 'reasonably': 3814, 'reasons': 3815, 'rebooking': 3816, 'reccommend': 3817, 'reccommended': 3818, 'receive': 3819, 'recent': 3820, 'recently': 3821, 'recepitionestyou': 3822, 'reception': 3823, 'recharge': 3824, 'reclaimed': 3825, 'recognised': 3826, 'recomend': 3827, 'recommend': 3828, 'recommendation': 3829, 'recommendations': 3830, 'recommended': 3831, 'rectify': 3832, 'red': 3833, 'redic': 3834, 'redo': 3835, 'reduce': 3836, 'reduced': 3837, 'reduces': 3838, 'redundant': 3839, 'refill': 3840, 'refillable': 3841, 'refilling': 3842, 'refills': 3843, 'refit': 3844, 'reflective': 3845, 'refocused': 3846, 'refreshing': 3847, 'refund': 3848, 'refurbish': 3849, 'refurbished': 3850, 'refurbishing': 3851, 'refurbishment': 3852, 'refurbishments': 3853, 'refurbrishment': 3854, 'refused': 3855, 'regardless': 3856, 'regards': 3857, 'regret': 3858, 'regular': 3859, 'regularly': 3860, 'rejoin': 3861, 'related': 3862, 'relations': 3863, 'relatively': 3864, 'relatives': 3865, 'relax': 3866, 'relaxedits': 3867, 'relieved': 3868, 'religion': 3869, 'religiously': 3870, 'relive': 3871, 'reluctanlty': 3872, 'reluctantly': 3873, 'rely': 3874, 'remained': 3875, 'remember': 3876, 'remembered': 3877, 'reminded': 3878, 'remortgaged': 3879, 'remote': 3880, 'rename': 3881, 'renamed': 3882, 'renew': 3883, 'renewing': 3884, 'renovating': 3885, 'renovation': 3886, 'renovations': 3887, 'rent': 3888, 'rental': 3889, 'rented': 3890, 'reocmmend': 3891, 'reopened': 3892, 'repair': 3893, 'repaired': 3894, 'repairing': 3895, 'repairs': 3896, 'repeat': 3897, 'repetitive': 3898, 'replica': 3899, 'replicas': 3900, 'reported': 3901, 'represent': 3902, 'requesting': 3903, 'requests': 3904, 'required': 3905, 'requirements': 3906, 'rer': 3907, 'research': 3908, 'researched': 3909, 'reservation': 3910, 'reservations': 3911, 'reserve': 3912, 'residence': 3913, 'resident': 3914, 'resolved': 3915, 'resort': 3916, 'resorts': 3917, 'resources': 3918, 'respect': 3919, 'respectful': 3920, 'respond': 3921, 'response': 3922, 'rest': 3923, 'restarted': 3924, 'restaurant': 3925, 'restaurants': 3926, 'restaurantsapart': 3927, 'restedfood': 3928, 'restos': 3929, 'restricted': 3930, 'restriction': 3931, 'restrictions': 3932, 'restroom': 3933, 'restrooms': 3934, 'resturant': 3935, 'resturants': 3936, 'restwe': 3937, 'result': 3938, 'resulted': 3939, 'resulting': 3940, 'retail': 3941, 'retains': 3942, 'retorted': 3943, 'retured': 3944, 'return': 3945, 'returned': 3946, 'returning': 3947, 'reunion': 3948, 'revenue': 3949, 'review': 3950, 'reviewer': 3951, 'reviewers': 3952, 'reviews': 3953, 'rewarding': 3954, 'rice': 3955, 'ride': 3956, 'ridea': 3957, 'rideif': 3958, 'rider': 3959, 'riders': 3960, 'rides': 3961, 'ridesbe': 3962, 'ridesbuy': 3963, 'ridesgo': 3964, 'ridesif': 3965, 'ridesqueues': 3966, 'ridessince': 3967, 'ridesthere': 3968, 'rideswe': 3969, 'ridethe': 3970, 'ridiculous': 3971, 'ridiculously': 3972, 'ridiculousthe': 3973, 'riding': 3974, 'right': 3975, 'rightly': 3976, 'ring': 3977, 'rinse': 3978, 'rip': 3979, 'ripped': 3980, 'risers': 3981, 'rising': 3982, 'risk': 3983, 'river': 3984, 'riverboat': 3985, 'road': 3986, 'roads': 3987, 'roamed': 3988, 'roaming': 3989, 'rob': 3990, 'robinson': 3991, 'robots': 3992, 'rock': 3993, 'rockin': 3994, 'rocks': 3995, 'rode': 3996, 'roeuntrip': 3997, 'role': 3998, 'roll': 3999, 'rollar': 4000, 'roller': 4001, 'rollercoaster': 4002, 'rollercoasters': 4003, 'rolls': 4004, 'romantic': 4005, 'room': 4006, 'rooms': 4007, 'roots': 4008, 'rope': 4009, 'ropes': 4010, 'roping': 4011, 'roughly': 4012, 'round': 4013, 'rounds': 4014, 'roundup': 4015, 'route': 4016, 'routine': 4017, 'roy': 4018, 'royal': 4019, 'royces': 4020, 'rsprings': 4021, 'rthen': 4022, 'rubbish': 4023, 'rubes': 4024, 'rude': 4025, 'rudebring': 4026, 'ruin': 4027, 'ruined': 4028, 'rule': 4029, 'rules': 4030, 'rulessave': 4031, 'run': 4032, 'runaway': 4033, 'rundisney': 4034, 'running': 4035, 'runs': 4036, 'rural': 4037, 'rush': 4038, 'rushed': 4039, 'rushes': 4040, 'rushing': 4041, 'russell': 4042, 'russia': 4043, 'rustler': 4044, 's': 4045, 'sachet': 4046, 'sad': 4047, 'sadly': 4048, 'safe': 4049, 'safely': 4050, 'safety': 4051, 'said': 4052, 'sailing': 4053, 'sake': 4054, 'sakes': 4055, 'salad': 4056, 'salads': 4057, 'sale': 4058, 'same': 4059, 'samecompletely': 4060, 'samey': 4061, 'san': 4062, 'sandwich': 4063, 'sandwiches': 4064, 'santa': 4065, 'sardines': 4066, 'sat': 4067, 'satisfying': 4068, 'saturday': 4069, 'sauce': 4070, 'saucer': 4071, 'save': 4072, 'saved': 4073, 'saves': 4074, 'saving': 4075, 'savings': 4076, 'savvy': 4077, 'saw': 4078, 'sawyer': 4079, 'sawyers': 4080, 'say': 4081, 'saying': 4082, 'sayings': 4083, 'says': 4084, 'scaffolding': 4085, 'scale': 4086, 'scaled': 4087, 'scaleif': 4088, 'scan': 4089, 'scanner': 4090, 'scanning': 4091, 'scared': 4092, 'scarier': 4093, 'scariest': 4094, 'scarry': 4095, 'scary': 4096, 'scenery': 4097, 'sceptical': 4098, 'scepticial': 4099, 'sceptics': 4100, 'schedule': 4101, 'scheduled': 4102, 'school': 4103, 'schools': 4104, 'scoot': 4105, 'scooters': 4106, 'scorching': 4107, 'screamin': 4108, 'screaming': 4109, 'screening': 4110, 'screens': 4111, 'script': 4112, 'sea': 4113, 'seam': 4114, 'search': 4115, 'searched': 4116, 'season': 4117, 'seasoni': 4118, 'seasonon': 4119, 'seasons': 4120, 'seat': 4121, 'seated': 4122, 'seating': 4123, 'seats': 4124, 'second': 4125, 'secondary': 4126, 'seconds': 4127, 'secrets': 4128, 'section': 4129, 'sections': 4130, 'secure': 4131, 'security': 4132, 'see': 4133, 'seeing': 4134, 'seek': 4135, 'seeking': 4136, 'seem': 4137, 'seemed': 4138, 'seems': 4139, 'seemust': 4140, 'seen': 4141, 'seethere': 4142, 'seeting': 4143, 'select': 4144, 'selected': 4145, 'selection': 4146, 'selections': 4147, 'selective': 4148, 'self': 4149, 'selfie': 4150, 'sell': 4151, 'selling': 4152, 'sells': 4153, 'send': 4154, 'sending': 4155, 'sends': 4156, 'senior': 4157, 'sensational': 4158, 'sense': 4159, 'sensible': 4160, 'sensitive': 4161, 'sent': 4162, 'separate': 4163, 'separately': 4164, 'sept': 4165, 'september': 4166, 'series': 4167, 'seriously': 4168, 'serval': 4169, 'serve': 4170, 'served': 4171, 'service': 4172, 'serviceable': 4173, 'services': 4174, 'serviette': 4175, 'serving': 4176, 'servings': 4177, 'session': 4178, 'sessions': 4179, 'set': 4180, 'sets': 4181, 'settle': 4182, 'seven': 4183, 'several': 4184, 'severe': 4185, 'severely': 4186, 'sex': 4187, 'sha': 4188, 'shade': 4189, 'shaded': 4190, 'shadei': 4191, 'shadow': 4192, 'shady': 4193, 'shaking': 4194, 'shambolic': 4195, 'shame': 4196, 'shape': 4197, 'shaped': 4198, 'share': 4199, 'sharing': 4200, 'sharp': 4201, 'she': 4202, 'shed': 4203, 'sheer': 4204, 'sheet': 4205, 'shewing': 4206, 'shine': 4207, 'ship': 4208, 'shirts': 4209, 'shock': 4210, 'shocking': 4211, 'shoddy': 4212, 'shoes': 4213, 'shoot': 4214, 'shooting': 4215, 'shop': 4216, 'shoppe': 4217, 'shopping': 4218, 'shops': 4219, 'shopsthe': 4220, 'short': 4221, 'shortearly': 4222, 'shorter': 4223, 'shortest': 4224, 'shortly': 4225, 'should': 4226, 'shoulder': 4227, 'shoulders': 4228, 'shouldersthe': 4229, 'shouldnt': 4230, 'shouldve': 4231, 'shout': 4232, 'shouting': 4233, 'shoutout': 4234, 'shove': 4235, 'shoved': 4236, 'shoving': 4237, 'show': 4238, 'showed': 4239, 'shower': 4240, 'showi': 4241, 'showing': 4242, 'shown': 4243, 'shownew': 4244, 'shows': 4245, 'showsthe': 4246, 'showsure': 4247, 'showthe': 4248, 'showtimes': 4249, 'showwow': 4250, 'shrug': 4251, 'shut': 4252, 'shuttle': 4253, 'shuttles': 4254, 'shy': 4255, 'sick': 4256, 'sickness': 4257, 'side': 4258, 'sidesevery': 4259, 'sight': 4260, 'sights': 4261, 'sign': 4262, 'signage': 4263, 'signal': 4264, 'signamazing': 4265, 'signature': 4266, 'significant': 4267, 'significantly': 4268, 'signing': 4269, 'signposted': 4270, 'signs': 4271, 'silly': 4272, 'similar': 4273, 'similarly': 4274, 'simple': 4275, 'simply': 4276, 'simultaneous': 4277, 'since': 4278, 'sincerely': 4279, 'sing': 4280, 'singapore': 4281, 'singaporean': 4282, 'singers': 4283, 'singing': 4284, 'single': 4285, 'sister': 4286, 'sit': 4287, 'site': 4288, 'sites': 4289, 'sitting': 4290, 'situated': 4291, 'situation': 4292, 'six': 4293, 'size': 4294, 'sized': 4295, 'skellington': 4296, 'skills': 4297, 'skin': 4298, 'skinsadly': 4299, 'skip': 4300, 'skipped': 4301, 'sky': 4302, 'skyrockets': 4303, 'slatted': 4304, 'slaughter': 4305, 'sleeping': 4306, 'sleepy': 4307, 'slept': 4308, 'slice': 4309, 'slightly': 4310, 'slinky': 4311, 'slippers': 4312, 'slips': 4313, 'slogging': 4314, 'slot': 4315, 'slow': 4316, 'slowed': 4317, 'slower': 4318, 'slowest': 4319, 'slowly': 4320, 'small': 4321, 'smaller': 4322, 'smallest': 4323, 'smallthe': 4324, 'smallworld': 4325, 'smart': 4326, 'smartly': 4327, 'smelled': 4328, 'smells': 4329, 'smile': 4330, 'smiles': 4331, 'smiling': 4332, 'smith': 4333, 'smoke': 4334, 'smoked': 4335, 'smokers': 4336, 'smokes': 4337, 'smoking': 4338, 'smokingoverall': 4339, 'snack': 4340, 'snacks': 4341, 'snacksif': 4342, 'snapped': 4343, 'snarling': 4344, 'snd': 4345, 'sneak': 4346, 'snow': 4347, 'snowed': 4348, 'so': 4349, 'soa': 4350, 'soaked': 4351, 'soarin': 4352, 'soaring': 4353, 'socal': 4354, 'soda': 4355, 'sold': 4356, 'soldiered': 4357, 'sole': 4358, 'solely': 4359, 'solid': 4360, 'soloists': 4361, 'solution': 4362, 'solve': 4363, 'solved': 4364, 'som': 4365, 'some': 4366, 'someblackout': 4367, 'someday': 4368, 'somehow': 4369, 'someone': 4370, 'something': 4371, 'sometimes': 4372, 'somewhat': 4373, 'somewhere': 4374, 'son': 4375, 'song': 4376, 'songs': 4377, 'sons': 4378, 'soon': 4379, 'sooner': 4380, 'sooo': 4381, 'soooo': 4382, 'sooooo': 4383, 'sooooooooo': 4384, 'sorely': 4385, 'sorry': 4386, 'sort': 4387, 'sorts': 4388, 'sound': 4389, 'sounds': 4390, 'soundsational': 4391, 'sourwill': 4392, 'southern': 4393, 'souveneirs': 4394, 'souvenier': 4395, 'souveniers': 4396, 'souvenir': 4397, 'souvenirs': 4398, 'space': 4399, 'spacecraft': 4400, 'spacemountain': 4401, 'spaces': 4402, 'spaceships': 4403, 'spacial': 4404, 'spanish': 4405, 'sparethe': 4406, 'spark': 4407, 'sparkling': 4408, 'sparrow': 4409, 'speak': 4410, 'speaker': 4411, 'speaking': 4412, 'speaks': 4413, 'special': 4414, 'specially': 4415, 'specific': 4416, 'specified': 4417, 'spectacle': 4418, 'spectacles': 4419, 'spectacular': 4420, 'spectacularbit': 4421, 'spectacularon': 4422, 'spectaculor': 4423, 'speed': 4424, 'spell': 4425, 'spend': 4426, 'spending': 4427, 'spends': 4428, 'spent': 4429, 'spider': 4430, 'spill': 4431, 'spin': 4432, 'spinning': 4433, 'spirit': 4434, 'spiritual': 4435, 'splash': 4436, 'split': 4437, 'spoil': 4438, 'spoiled': 4439, 'spoiling': 4440, 'spoilt': 4441, 'spoke': 4442, 'spoken': 4443, 'spoon': 4444, 'spot': 4445, 'spotless': 4446, 'spotlessly': 4447, 'spots': 4448, 'spotthings': 4449, 'spring': 4450, 'springs': 4451, 'sprinkles': 4452, 'sprinted': 4453, 'spur': 4454, 'spwak': 4455, 'square': 4456, 'squeeze': 4457, 'squeezed': 4458, 'st': 4459, 'staff': 4460, 'staffed': 4461, 'staffs': 4462, 'stage': 4463, 'stairs': 4464, 'stake': 4465, 'staleness': 4466, 'stall': 4467, 'stalls': 4468, 'stamped': 4469, 'stampede': 4470, 'stand': 4471, 'standard': 4472, 'standards': 4473, 'standing': 4474, 'stands': 4475, 'standstill': 4476, 'star': 4477, 'starbucks': 4478, 'stared': 4479, 'starenger': 4480, 'staring': 4481, 'stark': 4482, 'stars': 4483, 'start': 4484, 'started': 4485, 'starting': 4486, 'starts': 4487, 'startwe': 4488, 'starving': 4489, 'starwars': 4490, 'state': 4491, 'stated': 4492, 'statement': 4493, 'states': 4494, 'static': 4495, 'station': 4496, 'stationed': 4497, 'stationnote': 4498, 'stations': 4499, 'statue': 4500, 'statues': 4501, 'stay': 4502, 'stayed': 4503, 'staying': 4504, 'stayover': 4505, 'steamy': 4506, 'steep': 4507, 'stellar': 4508, 'stench': 4509, 'step': 4510, 'stepped': 4511, 'steps': 4512, 'stereotype': 4513, 'sterile': 4514, 'stick': 4515, 'sticker': 4516, 'stickers': 4517, 'sticking': 4518, 'sticks': 4519, 'stickssee': 4520, 'still': 4521, 'sting': 4522, 'stinks': 4523, 'stitch': 4524, 'stocked': 4525, 'stoller': 4526, 'stomach': 4527, 'stomaches': 4528, 'stone': 4529, 'stood': 4530, 'stop': 4531, 'stopped': 4532, 'stopping': 4533, 'storage': 4534, 'store': 4535, 'storeparis': 4536, 'stores': 4537, 'storesin': 4538, 'stories': 4539, 'storm': 4540, 'story': 4541, 'storybook': 4542, 'storytelling': 4543, 'straight': 4544, 'strange': 4545, 'strangers': 4546, 'strap': 4547, 'strategy': 4548, 'street': 4549, 'streets': 4550, 'stress': 4551, 'stressed': 4552, 'strewn': 4553, 'strict': 4554, 'strictly': 4555, 'stroll': 4556, 'strolled': 4557, 'stroller': 4558, 'strollers': 4559, 'strolling': 4560, 'strongly': 4561, 'strret': 4562, 'structure': 4563, 'struggle': 4564, 'struggleavoid': 4565, 'stuck': 4566, 'students': 4567, 'studio': 4568, 'studios': 4569, 'studiosright': 4570, 'study': 4571, 'studying': 4572, 'stuff': 4573, 'stuffed': 4574, 'stuffing': 4575, 'stuffy': 4576, 'stunk': 4577, 'stunning': 4578, 'stunt': 4579, 'style': 4580, 'sub': 4581, 'subcontinent': 4582, 'submarines': 4583, 'subs': 4584, 'substantially': 4585, 'substitute': 4586, 'subtle': 4587, 'subway': 4588, 'successfully': 4589, 'such': 4590, 'sucks': 4591, 'sudden': 4592, 'suddenly': 4593, 'suffice': 4594, 'suger': 4595, 'suggest': 4596, 'suggested': 4597, 'suggestion': 4598, 'suggestions': 4599, 'suggestionstake': 4600, 'suggests': 4601, 'suit': 4602, 'suitable': 4603, 'suitcase': 4604, 'suited': 4605, 'suites': 4606, 'suits': 4607, 'sum': 4608, 'summer': 4609, 'summers': 4610, 'summertime': 4611, 'sun': 4612, 'sunday': 4613, 'sunny': 4614, 'sunscreen': 4615, 'sunset': 4616, 'sunshine': 4617, 'super': 4618, 'superb': 4619, 'superbeware': 4620, 'superior': 4621, 'superlative': 4622, 'supermarket': 4623, 'supper': 4624, 'supply': 4625, 'supposed': 4626, 'sure': 4627, 'surely': 4628, 'surge': 4629, 'surly': 4630, 'surmised': 4631, 'surprise': 4632, 'surprised': 4633, 'surprises': 4634, 'surprisingly': 4635, 'surreal': 4636, 'surrounding': 4637, 'surveil': 4638, 'survived': 4639, 'suspect': 4640, 'suspended': 4641, 'swans': 4642, 'swap': 4643, 'swapped': 4644, 'swarming': 4645, 'sweater': 4646, 'sweating': 4647, 'sweep': 4648, 'sweet': 4649, 'sweets': 4650, 'swim': 4651, 'swimand': 4652, 'swinging': 4653, 'swipe': 4654, 'swiss': 4655, 'switch': 4656, 'switching': 4657, 'swiveling': 4658, 'symphony': 4659, 'synchronised': 4660, 'system': 4661, 'systematically': 4662, 'systematicallychildren': 4663, 't': 4664, 'table': 4665, 'tables': 4666, 'tackle': 4667, 'tad': 4668, 'tag': 4669, 'tagged': 4670, 'tahitian': 4671, 'tails': 4672, 'take': 4673, 'taken': 4674, 'takes': 4675, 'taking': 4676, 'takr': 4677, 'tale': 4678, 'talented': 4679, 'talk': 4680, 'talked': 4681, 'talking': 4682, 'tall': 4683, 'tame': 4684, 'tan': 4685, 'tap': 4686, 'tapered': 4687, 'tarzan': 4688, 'tarzans': 4689, 'taste': 4690, 'tasted': 4691, 'tasty': 4692, 'tat': 4693, 'tax': 4694, 'taxi': 4695, 'tea': 4696, 'teacups': 4697, 'team': 4698, 'teams': 4699, 'teapots': 4700, 'tear': 4701, 'tears': 4702, 'tearsfor': 4703, 'tech': 4704, 'technical': 4705, 'technologie': 4706, 'technology': 4707, 'teen': 4708, 'teenage': 4709, 'teenager': 4710, 'teenagers': 4711, 'teenages': 4712, 'teens': 4713, 'teeny': 4714, 'tell': 4715, 'telling': 4716, 'tells': 4717, 'temp': 4718, 'tempers': 4719, 'tempetures': 4720, 'temple': 4721, 'temporarily': 4722, 'temporary': 4723, 'ten': 4724, 'tend': 4725, 'tends': 4726, 'tents': 4727, 'ter': 4728, 'term': 4729, 'terms': 4730, 'terrace': 4731, 'terrible': 4732, 'terrific': 4733, 'territory': 4734, 'terror': 4735, 'terrors': 4736, 'test': 4737, 'tested': 4738, 'tfor': 4739, 'tha': 4740, 'than': 4741, 'thank': 4742, 'thankful': 4743, 'thankfully': 4744, 'thanks': 4745, 'that': 4746, 'thats': 4747, 'thatwdw': 4748, 'the': 4749, 'theater': 4750, 'theatre': 4751, 'theatrical': 4752, 'their': 4753, 'them': 4754, 'themalso': 4755, 'theme': 4756, 'themed': 4757, 'themeing': 4758, 'themeparks': 4759, 'themes': 4760, 'themfood': 4761, 'themif': 4762, 'theming': 4763, 'themselves': 4764, 'themselvesoh': 4765, 'themthe': 4766, 'themthey': 4767, 'then': 4768, 'ther': 4769, 'there': 4770, 'thereanother': 4771, 'thereday': 4772, 'therefore': 4773, 'thereplenty': 4774, 'theres': 4775, 'therethe': 4776, 'theretoo': 4777, 'therewe': 4778, 'therir': 4779, 'these': 4780, 'they': 4781, 'theyd': 4782, 'theyre': 4783, 'theyve': 4784, 'thier': 4785, 'thing': 4786, 'things': 4787, 'thingswe': 4788, 'think': 4789, 'thinking': 4790, 'thinsk': 4791, 'third': 4792, 'thirst': 4793, 'thirsty': 4794, 'thirteen': 4795, 'this': 4796, 'thna': 4797, 'thngs': 4798, 'tho': 4799, 'thor': 4800, 'thorough': 4801, 'thoroughfare': 4802, 'thoroughly': 4803, 'those': 4804, 'though': 4805, 'thought': 4806, 'thoughthe': 4807, 'thoughtphotopass': 4808, 'thousands': 4809, 'threaten': 4810, 'three': 4811, 'thrid': 4812, 'thrill': 4813, 'thrilled': 4814, 'thrilling': 4815, 'thrillsthere': 4816, 'through': 4817, 'throughout': 4818, 'thrown': 4819, 'thru': 4820, 'thunder': 4821, 'thur': 4822, 'thurs': 4823, 'thursday': 4824, 'thus': 4825, 'thw': 4826, 'tiana': 4827, 'tick': 4828, 'ticked': 4829, 'ticket': 4830, 'ticketing': 4831, 'ticketnetfr': 4832, 'tickets': 4833, 'ticketsoveralli': 4834, 'tide': 4835, 'tidy': 4836, 'tigger': 4837, 'tight': 4838, 'tiki': 4839, 'til': 4840, 'tiles': 4841, 'till': 4842, 'time': 4843, 'timed': 4844, 'timefood': 4845, 'timeget': 4846, 'timeif': 4847, 'timeless': 4848, 'timemaybe': 4849, 'timeour': 4850, 'timer': 4851, 'timers': 4852, 'times': 4853, 'timesfor': 4854, 'timeshare': 4855, 'timeslot': 4856, 'timesthere': 4857, 'timeswe': 4858, 'timethe': 4859, 'timewe': 4860, 'timing': 4861, 'timings': 4862, 'tinkerbell': 4863, 'tiny': 4864, 'tip': 4865, 'tips': 4866, 'tired': 4867, 'tires': 4868, 'tiring': 4869, 'tissues': 4870, 'title': 4871, 'tix': 4872, 'tlc': 4873, 'to': 4874, 'toa': 4875, 'toad': 4876, 'toads': 4877, 'toaster': 4878, 'today': 4879, 'todays': 4880, 'toddler': 4881, 'toddlers': 4882, 'together': 4883, 'tohave': 4884, 'toilet': 4885, 'toileteries': 4886, 'toilets': 4887, 'toiletsall': 4888, 'tokyo': 4889, 'told': 4890, 'tolerable': 4891, 'tolerated': 4892, 'tom': 4893, 'tommorowland': 4894, 'tomorrow': 4895, 'tomorrowland': 4896, 'ton': 4897, 'tone': 4898, 'tons': 4899, 'too': 4900, 'tooits': 4901, 'took': 4902, 'tool': 4903, 'toon': 4904, 'toontown': 4905, 'toontownnew': 4906, 'toooverall': 4907, 'toother': 4908, 'toowe': 4909, 'top': 4910, 'topasta': 4911, 'topped': 4912, 'tops': 4913, 'torn': 4914, 'tossed': 4915, 'tossthis': 4916, 'total': 4917, 'totalled': 4918, 'totally': 4919, 'tote': 4920, 'toted': 4921, 'tothe': 4922, 'tothere': 4923, 'touch': 4924, 'touches': 4925, 'tough': 4926, 'tour': 4927, 'toured': 4928, 'touring': 4929, 'tourist': 4930, 'tourists': 4931, 'tours': 4932, 'touts': 4933, 'tow': 4934, 'towards': 4935, 'towe': 4936, 'towels': 4937, 'tower': 4938, 'towers': 4939, 'town': 4940, 'towns': 4941, 'townthanks': 4942, 'toy': 4943, 'toys': 4944, 'track': 4945, 'tracked': 4946, 'traditional': 4947, 'traditionalists': 4948, 'traffic': 4949, 'train': 4950, 'trained': 4951, 'training': 4952, 'trains': 4953, 'trak': 4954, 'tram': 4955, 'transfer': 4956, 'transferred': 4957, 'transfers': 4958, 'transform': 4959, 'translated': 4960, 'translation': 4961, 'transport': 4962, 'transportation': 4963, 'transported': 4964, 'trapduring': 4965, 'trapped': 4966, 'travel': 4967, 'traveled': 4968, 'traveling': 4969, 'travelled': 4970, 'travelling': 4971, 'traverse': 4972, 'treasure': 4973, 'treat': 4974, 'treated': 4975, 'treatedthey': 4976, 'treaters': 4977, 'treating': 4978, 'treatment': 4979, 'treats': 4980, 'trecking': 4981, 'tree': 4982, 'treehouse': 4983, 'treehouseno': 4984, 'trees': 4985, 'trek': 4986, 'trekking': 4987, 'trick': 4988, 'tricks': 4989, 'tried': 4990, 'trip': 4991, 'triplets': 4992, 'trips': 4993, 'triumphat': 4994, 'tron': 4995, 'troopers': 4996, 'troubles': 4997, 'truck': 4998, 'true': 4999, 'truekids': 5000, 'trully': 5001, 'truly': 5002, 'trust': 5003, 'try': 5004, 'trying': 5005, 'ts': 5006, 'tsim': 5007, 'tsui': 5008, 'tub': 5009, 'tuckered': 5010, 'tue': 5011, 'tues': 5012, 'tuesday': 5013, 'tuna': 5014, 'tunder': 5015, 'tunes': 5016, 'tunnelscant': 5017, 'turkey': 5018, 'turn': 5019, 'turned': 5020, 'turning': 5021, 'turns': 5022, 'turnstiles': 5023, 'tv': 5024, 'twain': 5025, 'tweenagers': 5026, 'tweens': 5027, 'twenties': 5028, 'twenty': 5029, 'twice': 5030, 'twilight': 5031, 'twins': 5032, 'twist': 5033, 'two': 5034, 'type': 5035, 'types': 5036, 'typhoon': 5037, 'typical': 5038, 'typically': 5039, 'u': 5040, 'uglier': 5041, 'ugly': 5042, 'uk': 5043, 'ukour': 5044, 'ukthe': 5045, 'umbrella': 5046, 'umbrellas': 5047, 'un': 5048, 'unable': 5049, 'unacceptable': 5050, 'unavailable': 5051, 'unbearable': 5052, 'unbelievable': 5053, 'uncomfortable': 5054, 'under': 5055, 'undergoing': 5056, 'understand': 5057, 'understanding': 5058, 'understood': 5059, 'undertaken': 5060, 'underwhelmed': 5061, 'undescribable': 5062, 'uneventfulvisited': 5063, 'unexciting': 5064, 'unexpected': 5065, 'unexpectedly': 5066, 'unforgetable': 5067, 'unforgettable': 5068, 'unforgivable': 5069, 'unfortanely': 5070, 'unfortunately': 5071, 'unfriendly': 5072, 'unfriendlystressed': 5073, 'unhappy': 5074, 'unhelpful': 5075, 'unimaginable': 5076, 'unions': 5077, 'unique': 5078, 'uniquely': 5079, 'universal': 5080, 'unless': 5081, 'unlike': 5082, 'unlikely': 5083, 'unlimited': 5084, 'unlit': 5085, 'unmotivated': 5086, 'unnecessarily': 5087, 'unnecessary': 5088, 'unofficial': 5089, 'unorganised': 5090, 'unparalleled': 5091, 'unpleasant': 5092, 'unprofessionaloh': 5093, 'unreal': 5094, 'unreasonable': 5095, 'unruly': 5096, 'unsure': 5097, 'unsuspecting': 5098, 'until': 5099, 'untill': 5100, 'up': 5101, 'upbeat': 5102, 'update': 5103, 'updated': 5104, 'updates': 5105, 'updating': 5106, 'upgrade': 5107, 'upkeep': 5108, 'upon': 5109, 'ups': 5110, 'upset': 5111, 'upseting': 5112, 'upsetting': 5113, 'upside': 5114, 'upswhen': 5115, 'upthunder': 5116, 'ur': 5117, 'urge': 5118, 'urges': 5119, 'us': 5120, 'usa': 5121, 'usafirstly': 5122, 'usage': 5123, 'use': 5124, 'used': 5125, 'useless': 5126, 'users': 5127, 'uses': 5128, 'ushave': 5129, 'using': 5130, 'usthis': 5131, 'usual': 5132, 'usually': 5133, 'utilize': 5134, 'utilizing': 5135, 'utopia': 5136, 'vacation': 5137, 'vacations': 5138, 'vader': 5139, 'vain': 5140, 'valee': 5141, 'valid': 5142, 'vallee': 5143, 'value': 5144, 'valuebuffalo': 5145, 'valuebuffet': 5146, 'varied': 5147, 'varies': 5148, 'variety': 5149, 'varietygoodies': 5150, 'variey': 5151, 'various': 5152, 'vary': 5153, 'vast': 5154, 've': 5155, 'veg': 5156, 'vegas': 5157, 'vegitarian': 5158, 'vehicle': 5159, 'ventsjust': 5160, 'ventured': 5161, 'venues': 5162, 'verbally': 5163, 'verdict': 5164, 'verging': 5165, 'versed': 5166, 'version': 5167, 'versions': 5168, 'versionthe': 5169, 'versus': 5170, 'very': 5171, 'vestiges': 5172, 'veteran': 5173, 'via': 5174, 'vibe': 5175, 'vicnitiygenerally': 5176, 'view': 5177, 'viewand': 5178, 'viewing': 5179, 'views': 5180, 'villa': 5181, 'village': 5182, 'villages': 5183, 'villagetickets': 5184, 'villiage': 5185, 'viosit': 5186, 'vip': 5187, 'virtually': 5188, 'visit': 5189, 'visited': 5190, 'visitfantastic': 5191, 'visiting': 5192, 'visitmost': 5193, 'visitng': 5194, 'visitor': 5195, 'visitors': 5196, 'visits': 5197, 'visor': 5198, 'visted': 5199, 'visual': 5200, 'vital': 5201, 'voice': 5202, 'volume': 5203, 'voucher': 5204, 'vouchers': 5205, 'vry': 5206, 'vulnerable': 5207, 'w': 5208, 'waffles': 5209, 'wagon': 5210, 'waisted': 5211, 'wait': 5212, 'waita': 5213, 'waited': 5214, 'waiting': 5215, 'waits': 5216, 'wake': 5217, 'waking': 5218, 'walk': 5219, 'walked': 5220, 'walker': 5221, 'walking': 5222, 'walkingalso': 5223, 'walks': 5224, 'walkwaysi': 5225, 'wall': 5226, 'wallet': 5227, 'walling': 5228, 'walt': 5229, 'waltvdisney': 5230, 'wander': 5231, 'wandered': 5232, 'wandering': 5233, 'want': 5234, 'wanted': 5235, 'wantedwith': 5236, 'wanting': 5237, 'wants': 5238, 'war': 5239, 'wards': 5240, 'ware': 5241, 'warm': 5242, 'warmer': 5243, 'warmth': 5244, 'warning': 5245, 'warnings': 5246, 'wars': 5247, 'was': 5248, 'wasn': 5249, 'wasnt': 5250, 'wason': 5251, 'waste': 5252, 'wasted': 5253, 'wasting': 5254, 'watch': 5255, 'watched': 5256, 'watching': 5257, 'water': 5258, 'wateri': 5259, 'waterproofs': 5260, 'watersome': 5261, 'wave': 5262, 'waving': 5263, 'wax': 5264, 'way': 5265, 'wayline': 5266, 'waymy': 5267, 'ways': 5268, 'wayspent': 5269, 'wayt': 5270, 'wd': 5271, 'wdw': 5272, 'we': 5273, 'wear': 5274, 'wearing': 5275, 'weather': 5276, 'webs': 5277, 'website': 5278, 'websites': 5279, 'wed': 5280, 'wedding': 5281, 'wednesday': 5282, 'wee': 5283, 'week': 5284, 'weekday': 5285, 'weekdayfirst': 5286, 'weekdays': 5287, 'weeked': 5288, 'weekend': 5289, 'weekends': 5290, 'weekendthe': 5291, 'weeks': 5292, 'weenie': 5293, 'weigh': 5294, 'weird': 5295, 'welcome': 5296, 'well': 5297, 'wellafter': 5298, 'wellcongratulations': 5299, 'wellgrizzly': 5300, 'welli': 5301, 'wellif': 5302, 'welllikes': 5303, 'wellthe': 5304, 'went': 5305, 'were': 5306, 'weren': 5307, 'werent': 5308, 'west': 5309, 'western': 5310, 'wet': 5311, 'wether': 5312, 'weve': 5313, 'wh': 5314, 'what': 5315, 'whatever': 5316, 'whats': 5317, 'whatsoever': 5318, 'wheeelcair': 5319, 'wheel': 5320, 'wheelchair': 5321, 'wheelchairs': 5322, 'wheeled': 5323, 'when': 5324, 'whenever': 5325, 'where': 5326, 'whereas': 5327, 'whereby': 5328, 'wherever': 5329, 'whether': 5330, 'which': 5331, 'whiffs': 5332, 'while': 5333, 'whilst': 5334, 'whimsical': 5335, 'whip': 5336, 'whit': 5337, 'white': 5338, 'who': 5339, 'whole': 5340, 'wholebefore': 5341, 'whos': 5342, 'why': 5343, 'wi': 5344, 'wide': 5345, 'widely': 5346, 'widened': 5347, 'wider': 5348, 'wife': 5349, 'wifedaughter': 5350, 'wifi': 5351, 'wifis': 5352, 'wild': 5353, 'will': 5354, 'willing': 5355, 'window': 5356, 'windows': 5357, 'winds': 5358, 'windy': 5359, 'winnie': 5360, 'wins': 5361, 'winter': 5362, 'winters': 5363, 'wise': 5364, 'wisely': 5365, 'wish': 5366, 'wished': 5367, 'wishing': 5368, 'with': 5369, 'within': 5370, 'without': 5371, 'withstand': 5372, 'witness': 5373, 'witnessed': 5374, 'wnated': 5375, 'woefully': 5376, 'woman': 5377, 'wonder': 5378, 'wonderful': 5379, 'wonderfully': 5380, 'wonderfulthe': 5381, 'wonderland': 5382, 'wonderous': 5383, 'wondrous': 5384, 'wont': 5385, 'wontt': 5386, 'wood': 5387, 'woody': 5388, 'woodys': 5389, 'word': 5390, 'words': 5391, 'wore': 5392, 'woried': 5393, 'work': 5394, 'worked': 5395, 'workers': 5396, 'working': 5397, 'workingeven': 5398, 'works': 5399, 'worksloved': 5400, 'world': 5401, 'worlds': 5402, 'worn': 5403, 'worried': 5404, 'worry': 5405, 'worse': 5406, 'worsei': 5407, 'worst': 5408, 'worth': 5409, 'worthwhile': 5410, 'worthy': 5411, 'wot': 5412, 'wouderful': 5413, 'would': 5414, 'wouldn': 5415, 'wouldnt': 5416, 'wow': 5417, 'wownobody': 5418, 'wranglers': 5419, 'wrap': 5420, 'write': 5421, 'written': 5422, 'writting': 5423, 'wrong': 5424, 'wroth': 5425, 'wwwfnacticketscom': 5426, 'x': 5427, 'xo': 5428, 'yard': 5429, 'yay': 5430, 'yeah': 5431, 'year': 5432, 'yearly': 5433, 'years': 5434, 'yelled': 5435, 'yelling': 5436, 'yep': 5437, 'yes': 5438, 'yesterday': 5439, 'yet': 5440, 'yo': 5441, 'yonder': 5442, 'york': 5443, 'you': 5444, 'youd': 5445, 'youde': 5446, 'youll': 5447, 'youmanners': 5448, 'young': 5449, 'younger': 5450, 'youngest': 5451, 'youon': 5452, 'your': 5453, 'youre': 5454, 'yourself': 5455, 'youthe': 5456, 'youthful': 5457, 'youve': 5458, 'yr': 5459, 'yrs': 5460, 'ytou': 5461, 'yugh': 5462, 'zar': 5463, 'zone': 5464, 'zoo': 5465}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "updated_word_index = {word: to_categorical(index, num_classes=len(unique_word_index),dtype='float64') for word, index in unique_word_index.items()}"
      ],
      "metadata": {
        "id": "vhemi8lP6gRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(updated_word_index.items())[:3])"
      ],
      "metadata": {
        "id": "K_HVmmckUmP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2a53a5-b57e-46bc-bfa8-520de06c99cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', array([1., 0., 0., ..., 0., 0., 0.])), ('aa', array([0., 1., 0., ..., 0., 0., 0.])), ('aaa', array([0., 0., 1., ..., 0., 0., 0.]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now i convert the words in the reviews in a numpy array. In doing this i used a function that:\n",
        "\n",
        "- defines the dimensions of the array (num_words x num_features), notice that num_features is the embedding dimension;\n",
        "\n",
        "- fills each row (correspondent to the word) of the array with the right vector, by comparing the word from the review with the dictionary.\n",
        "\n",
        "Because the mean of words per review in the exam text was said to be 130, i decided to put the num_words to 130 for each review. In this way the reviews with less than 130 words will be filled with zero vectors, and the ones with more will be cut at 130. This could not be the more efficient solution, but it was the most accurate because the network requires a tensor which has a not ambigous shape."
      ],
      "metadata": {
        "id": "ta5hi1XdnhGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(filtered_words, word_vector_dict):\n",
        "    num_words = len(filtered_words)\n",
        "    num_features = len(next(iter(word_vector_dict.values())))\n",
        "    one_hot_vectors = np.zeros((num_words, num_features)) #zeros array\n",
        "\n",
        "    for i, word in enumerate(filtered_words): #fill the array\n",
        "        if word in word_vector_dict:\n",
        "            one_hot_vectors[i] = word_vector_dict[word]\n",
        "\n",
        "    return one_hot_vectors\n",
        "\n",
        "# Apply the conversion function to the 'Review_Text' column\n",
        "df_valid['Review_Text'] = df_valid['Review_Text'].apply(lambda x: x[:130]) #all reviews max 130 words\n",
        "df_valid['One_hot'] = df_valid['Review_Text'].apply(lambda x: convert_to_one_hot(x, updated_word_index))"
      ],
      "metadata": {
        "id": "H8opmQaRYlQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, i create a column dor the one hot vectors. Here is the actual dataset:"
      ],
      "metadata": {
        "id": "PK5r1IFaqA6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.head()"
      ],
      "metadata": {
        "id": "WhEaJtCFoI4-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c46a67c9-db82-4850-ad98-bf8a01a62177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Review_ID  Rating Year_Month Reviewer_Location  \\\n",
              "32236  474897517       5    2016-11           Ireland   \n",
              "17940  302685040       5     2015-8     United States   \n",
              "21184  225785637       5     2014-8            Canada   \n",
              "20356  241984006       4    2014-11            Canada   \n",
              "38404  201442095       1    missing    United Kingdom   \n",
              "\n",
              "                                             Review_Text  \\\n",
              "32236  visit every last year for my birthday as was a...   \n",
              "17940  for pete know what getting into when you go bu...   \n",
              "21184  the prrice keeps going up but how can you not ...   \n",
              "20356  time being to the the racing was very fun and ...   \n",
              "38404  loads of others rides kept closing for the mid...   \n",
              "\n",
              "                      Branch  word_count  \\\n",
              "32236       Disneyland_Paris         144   \n",
              "17940  Disneyland_California         105   \n",
              "21184  Disneyland_California         120   \n",
              "20356  Disneyland_California         118   \n",
              "38404       Disneyland_Paris         116   \n",
              "\n",
              "                                                 One_hot  \n",
              "32236  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "17940  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "21184  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "20356  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "38404  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-c92c7152-2b71-47da-acbe-ae245487536d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Year_Month</th>\n",
              "      <th>Reviewer_Location</th>\n",
              "      <th>Review_Text</th>\n",
              "      <th>Branch</th>\n",
              "      <th>word_count</th>\n",
              "      <th>One_hot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32236</th>\n",
              "      <td>474897517</td>\n",
              "      <td>5</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>visit every last year for my birthday as was a...</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>144</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17940</th>\n",
              "      <td>302685040</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-8</td>\n",
              "      <td>United States</td>\n",
              "      <td>for pete know what getting into when you go bu...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>105</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21184</th>\n",
              "      <td>225785637</td>\n",
              "      <td>5</td>\n",
              "      <td>2014-8</td>\n",
              "      <td>Canada</td>\n",
              "      <td>the prrice keeps going up but how can you not ...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>120</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20356</th>\n",
              "      <td>241984006</td>\n",
              "      <td>4</td>\n",
              "      <td>2014-11</td>\n",
              "      <td>Canada</td>\n",
              "      <td>time being to the the racing was very fun and ...</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>118</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38404</th>\n",
              "      <td>201442095</td>\n",
              "      <td>1</td>\n",
              "      <td>missing</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>loads of others rides kept closing for the mid...</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>116</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c92c7152-2b71-47da-acbe-ae245487536d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ae53c0ff-2bad-4e6e-a569-7bb16ccd21c5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae53c0ff-2bad-4e6e-a569-7bb16ccd21c5')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ae53c0ff-2bad-4e6e-a569-7bb16ccd21c5 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c92c7152-2b71-47da-acbe-ae245487536d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c92c7152-2b71-47da-acbe-ae245487536d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As said, each review is an array num_words x embedding_dimension"
      ],
      "metadata": {
        "id": "V-O0CL2Eq2Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['One_hot']:\n",
        "  print(i.shape)"
      ],
      "metadata": {
        "id": "2A9fBl_YQwvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cdddc0-5e41-4e45-8ab4-1866e1a55c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n",
            "(130, 5466)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we have the embedding, so the column with the raw text is no more useful!"
      ],
      "metadata": {
        "id": "SrkLNDouq-2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = df_valid.drop(['Review_Text'], axis=1)"
      ],
      "metadata": {
        "id": "cOEpB0GgEshp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.head()"
      ],
      "metadata": {
        "id": "I5HBYmBJEztd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e3d501ad-1341-4da8-b84f-dd69c24dce79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Review_ID  Rating Year_Month Reviewer_Location                 Branch  \\\n",
              "32236  474897517       5    2016-11           Ireland       Disneyland_Paris   \n",
              "17940  302685040       5     2015-8     United States  Disneyland_California   \n",
              "21184  225785637       5     2014-8            Canada  Disneyland_California   \n",
              "20356  241984006       4    2014-11            Canada  Disneyland_California   \n",
              "38404  201442095       1    missing    United Kingdom       Disneyland_Paris   \n",
              "\n",
              "       word_count                                            One_hot  \n",
              "32236         144  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "17940         105  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "21184         120  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "20356         118  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "38404         116  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-8ad6491c-08aa-46bd-813a-0e67671e29dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Year_Month</th>\n",
              "      <th>Reviewer_Location</th>\n",
              "      <th>Branch</th>\n",
              "      <th>word_count</th>\n",
              "      <th>One_hot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32236</th>\n",
              "      <td>474897517</td>\n",
              "      <td>5</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>144</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17940</th>\n",
              "      <td>302685040</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-8</td>\n",
              "      <td>United States</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>105</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21184</th>\n",
              "      <td>225785637</td>\n",
              "      <td>5</td>\n",
              "      <td>2014-8</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>120</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20356</th>\n",
              "      <td>241984006</td>\n",
              "      <td>4</td>\n",
              "      <td>2014-11</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>118</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38404</th>\n",
              "      <td>201442095</td>\n",
              "      <td>1</td>\n",
              "      <td>missing</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>116</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ad6491c-08aa-46bd-813a-0e67671e29dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-f50ea40d-b33d-4179-974e-f546900ecc56\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f50ea40d-b33d-4179-974e-f546900ecc56')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-f50ea40d-b33d-4179-974e-f546900ecc56 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ad6491c-08aa-46bd-813a-0e67671e29dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ad6491c-08aa-46bd-813a-0e67671e29dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the look of the dataset with the embedding done on the reviews. However, as i said in the written part, i will use also the other features (if i have informations i generally prefer to not throw them away, even tought i recognize that the crucial role is played by the reviews)."
      ],
      "metadata": {
        "id": "aj_4YU0jrK6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing**\n",
        "For the other features, this is the setting"
      ],
      "metadata": {
        "id": "3-qE36Tvb83z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n"
      ],
      "metadata": {
        "id": "hg-g4yZAb7AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, i split the 'Year_Month' column in one 'Year' and one 'Month' column, in order to allow keras to process the data. For the same reason, after i will also convert the values in the format 'Int64', from 'object'."
      ],
      "metadata": {
        "id": "FgbqRA6bdK3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid[['Year', 'Month']] = df_valid['Year_Month'].str.split('-', expand=True)\n",
        "df_valid = df_valid.drop(['Year_Month'], axis=1)\n",
        "df_valid.head()"
      ],
      "metadata": {
        "id": "WsrfhPum4Z4s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cc60b60d-5296-4403-9487-c9c4b139cb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Review_ID  Rating Reviewer_Location                 Branch  word_count  \\\n",
              "32236  474897517       5           Ireland       Disneyland_Paris         144   \n",
              "17940  302685040       5     United States  Disneyland_California         105   \n",
              "21184  225785637       5            Canada  Disneyland_California         120   \n",
              "20356  241984006       4            Canada  Disneyland_California         118   \n",
              "38404  201442095       1    United Kingdom       Disneyland_Paris         116   \n",
              "\n",
              "                                                 One_hot     Year Month  \n",
              "32236  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...     2016    11  \n",
              "17940  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...     2015     8  \n",
              "21184  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...     2014     8  \n",
              "20356  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...     2014    11  \n",
              "38404  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  missing  None  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-0a840621-6f08-4565-bcf4-ce8935a4a9a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Reviewer_Location</th>\n",
              "      <th>Branch</th>\n",
              "      <th>word_count</th>\n",
              "      <th>One_hot</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32236</th>\n",
              "      <td>474897517</td>\n",
              "      <td>5</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>144</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2016</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17940</th>\n",
              "      <td>302685040</td>\n",
              "      <td>5</td>\n",
              "      <td>United States</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>105</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2015</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21184</th>\n",
              "      <td>225785637</td>\n",
              "      <td>5</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>120</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2014</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20356</th>\n",
              "      <td>241984006</td>\n",
              "      <td>4</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>118</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2014</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38404</th>\n",
              "      <td>201442095</td>\n",
              "      <td>1</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>116</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>missing</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a840621-6f08-4565-bcf4-ce8935a4a9a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-01190909-3674-43d2-ab67-47f1834c726e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01190909-3674-43d2-ab67-47f1834c726e')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-01190909-3674-43d2-ab67-47f1834c726e button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a840621-6f08-4565-bcf4-ce8935a4a9a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a840621-6f08-4565-bcf4-ce8935a4a9a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important point is that some of the reviews are not providing full information about the data. This because maybe there was the year and not the month, or viceversa, or both. The point is that after the splitting of Y-M some missing values arise, as we can see below. This will be taken into account in the processing steps done after."
      ],
      "metadata": {
        "id": "_LyNiNSAdf_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uizfGkWpujRf",
        "outputId": "94fe13ae-cb51-4e7b-ff13-0ff9d8069146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 450 entries, 32236 to 22598\n",
            "Data columns (total 8 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Review_ID          450 non-null    int64 \n",
            " 1   Rating             450 non-null    int64 \n",
            " 2   Reviewer_Location  450 non-null    object\n",
            " 3   Branch             450 non-null    object\n",
            " 4   word_count         450 non-null    int64 \n",
            " 5   One_hot            450 non-null    object\n",
            " 6   Year               450 non-null    object\n",
            " 7   Month              418 non-null    object\n",
            "dtypes: int64(3), object(5)\n",
            "memory usage: 31.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the most important part of the features preprocessing (apart from the text part). I will apply the following trasformations to the columns:\n",
        "\n",
        "- Location: first of all, i applied the one-hot encoding in the classical sklearn way, then, because there were too many unique locations, i reduced the dimensionality with the TruncatedSVD (i couldn't use the PCA as i said in the written part because the input was sparse);\n",
        "\n",
        "- Branch: because there were just two branches, the one-hot encoding was enough in this case;\n",
        "\n",
        "- Year and Month: for these two columns first of all i applied an imputer to fill missing values, in fact by applying the convertion to Int64 some other missing values arise. Then i scaled them with standard scaler. This step can be very important to improve the model overall stability during training."
      ],
      "metadata": {
        "id": "DrY0yDkIeZEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "pip = Pipeline([\n",
        "    ('imp', KNNImputer()),\n",
        "    ('scaler', StandardScaler() ),\n",
        "])\n",
        "\n",
        "pipl = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('dim', TruncatedSVD(n_components=20)),\n",
        "])\n",
        "\n",
        "column_transformer = ColumnTransformer(transformers=[\n",
        "    ('pip_l', pipl, ['Reviewer_Location']),\n",
        "    ('encoder', OneHotEncoder(), ['Branch']),\n",
        "    ('imp_scal', pip, ['Year', 'Month'])],  # Specify the transformer and the column(s) to apply it on\n",
        "    remainder='passthrough',verbose_feature_names_out=False)  # Pass through any remaining columns as is\n",
        "\n",
        "columns_to_convert = ['Year', 'Month']\n",
        "df_valid[columns_to_convert] = df_valid[columns_to_convert].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\n"
      ],
      "metadata": {
        "id": "8KRaBYdI_AZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Model**\n",
        "\n",
        "In this section there is the most important part of the project, so the creation of the model. First of all i split the dataset between training and test set, with an 80% of instances in the first. I also put some random state and split in base of the proportion of y (with stratify). Before splitting, i remove the word_count column, as it's no more useful."
      ],
      "metadata": {
        "id": "E530fQZAhGVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_valid.drop(['Rating','word_count'], axis=1)\n",
        "y = df_valid['Rating']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=101, stratify=y)"
      ],
      "metadata": {
        "id": "Yg7d_o4w_y7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice, the pipeline isn't applied yet, so we still have the datas as before."
      ],
      "metadata": {
        "id": "XwdbII_Gi7hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "t4jv0aPBFeCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "03fb8a13-a611-4c92-efc4-08312f175ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Review_ID Reviewer_Location                 Branch  \\\n",
              "3144   437701069             Macau    Disneyland_HongKong   \n",
              "35393  307281324    United Kingdom       Disneyland_Paris   \n",
              "7487   188079803      South Africa    Disneyland_HongKong   \n",
              "29373  643311010    United Kingdom       Disneyland_Paris   \n",
              "4459   360407585    United Kingdom    Disneyland_HongKong   \n",
              "...          ...               ...                    ...   \n",
              "28838   10073111         Australia  Disneyland_California   \n",
              "31270  531328489           Ireland       Disneyland_Paris   \n",
              "3280   428989422     United States    Disneyland_HongKong   \n",
              "40373  146001483    United Kingdom       Disneyland_Paris   \n",
              "37282  238561384    United Kingdom       Disneyland_Paris   \n",
              "\n",
              "                                                 One_hot  Year  Month  \n",
              "3144   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2015     12  \n",
              "35393  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2015      8  \n",
              "7487   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2013     12  \n",
              "29373  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2018     12  \n",
              "4459   [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2016      3  \n",
              "...                                                  ...   ...    ...  \n",
              "28838  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  <NA>   <NA>  \n",
              "31270  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  <NA>   <NA>  \n",
              "3280   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2016      8  \n",
              "40373  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2012     11  \n",
              "37282  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  2014     10  \n",
              "\n",
              "[360 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9335d75e-f3ea-427c-a100-7e16f4ce030b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>Reviewer_Location</th>\n",
              "      <th>Branch</th>\n",
              "      <th>One_hot</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3144</th>\n",
              "      <td>437701069</td>\n",
              "      <td>Macau</td>\n",
              "      <td>Disneyland_HongKong</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2015</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35393</th>\n",
              "      <td>307281324</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2015</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7487</th>\n",
              "      <td>188079803</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>Disneyland_HongKong</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2013</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29373</th>\n",
              "      <td>643311010</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4459</th>\n",
              "      <td>360407585</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_HongKong</td>\n",
              "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28838</th>\n",
              "      <td>10073111</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Disneyland_California</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31270</th>\n",
              "      <td>531328489</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3280</th>\n",
              "      <td>428989422</td>\n",
              "      <td>United States</td>\n",
              "      <td>Disneyland_HongKong</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2016</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40373</th>\n",
              "      <td>146001483</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37282</th>\n",
              "      <td>238561384</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>Disneyland_Paris</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>2014</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>360 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9335d75e-f3ea-427c-a100-7e16f4ce030b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-dbba2538-a761-45f7-bc01-9e560b14d739\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dbba2538-a761-45f7-bc01-9e560b14d739')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-dbba2538-a761-45f7-bc01-9e560b14d739 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9335d75e-f3ea-427c-a100-7e16f4ce030b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9335d75e-f3ea-427c-a100-7e16f4ce030b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Adaptation of the training set**\n",
        "\n",
        "As i described in the test, the model consists in:\n",
        "\n",
        "- an RNN which processes the single reviews with 130 time steps and produces a compact representation of each review text;\n",
        "\n",
        "- an MLP that takes as input the concatenation between the compact representation produced from the RNN and the remaining features.\n",
        "\n",
        "This means that the model takes two different inputs in two differen moments!\n",
        "It is possible to do so, but i had to split the dataset in:\n",
        "\n",
        "- one part with only the embedding column 'One_Hot';\n",
        "- another part with all the other features.\n",
        "\n",
        "This is done here:"
      ],
      "metadata": {
        "id": "l-sIvk6QjPtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_features = X_train.drop(['One_hot'], axis=1)\n",
        "X_train_reviews = X_train['One_hot']\n",
        "X_test_features = X_test.drop(['One_hot'], axis=1)\n",
        "X_test_reviews = X_test['One_hot']"
      ],
      "metadata": {
        "id": "ZgpcAFsWFhYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now i process the part of the features, simply by applying the column transformer defined above and then converting everything into a tensor, to be used from the model. At the end i have a 560 x 26 tensor for the MLP."
      ],
      "metadata": {
        "id": "0lX70SBXkVkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fearues_def = column_transformer.fit_transform(X_train_features)\n",
        "X_train_fearues_def_t = tf.convert_to_tensor(X_train_fearues_def)\n",
        "X_test_fearues_def = column_transformer.fit_transform(X_test_features)\n",
        "X_test_fearues_def_t = tf.convert_to_tensor(X_test_fearues_def)"
      ],
      "metadata": {
        "id": "6KEGxUdfT7RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For what regards the embedding column, the situation is different.\n",
        "First, i had to convert the pd.series object (the column) into a list, in order to be able to access the single items (the reviews). This was done in order to transform the column in an array with dimension 560 x 130 x 6967 with np.reshape, and then into a tensor with the same dimensions. In this way the data can be used as input for the RNN."
      ],
      "metadata": {
        "id": "OlwirkOWknab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words, encoding_dim = X_train_reviews.iloc[0].shape\n",
        "X_train_reviews = X_train_reviews.tolist()\n",
        "X_train_reviews_array = np.reshape(X_train_reviews, (len(X_train), num_words, encoding_dim))\n",
        "X_train_reviews_tensor = tf.convert_to_tensor(X_train_reviews_array)\n",
        "X_test_reviews = X_test_reviews.tolist()\n",
        "X_test_reviews_array = np.reshape(X_test_reviews, (len(X_test), num_words, encoding_dim))\n",
        "X_test_reviews_tensor = tf.convert_to_tensor(X_test_reviews_array)"
      ],
      "metadata": {
        "id": "OAkHtiTJUjP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reviews_tensor.shape"
      ],
      "metadata": {
        "id": "zqEmX4J3UGU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6fea61-1af9-4799-c3ff-10e82df3eec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([360, 130, 5466])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target column has to firstly be subtracted by 1, in order to match with the positions of the output layer. Then i get the num_classes for later."
      ],
      "metadata": {
        "id": "Jx4kJ9pIlgJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_traindef = y_train.apply(lambda x: x - 1)\n",
        "y_traindef.unique()\n",
        "num_classes = len(y_traindef.unique())\n",
        "y_train_dense = tf.convert_to_tensor(y_traindef)\n",
        "y_testdef = y_test.apply(lambda x: x - 1)\n",
        "y_testdef.unique()\n",
        "num_classes = len(y_testdef.unique())\n",
        "y_test_dense = tf.convert_to_tensor(y_testdef)"
      ],
      "metadata": {
        "id": "Dxb-cCfCz3WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Bidirectional, GRU"
      ],
      "metadata": {
        "id": "mJ_kPKIoHw5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Network**\n",
        "\n",
        "The first part of the network is the recurrent. I define an input layer with shape (nwords,encode_dim). This means that then the recurrent layer will have exactly nwords time steps, and in each one it will analyze a vector of length encoding_dim.\n",
        "\n",
        "The recurrent layer i used was a Bidirectional, this because ideally, in textual data, at time step t, both previous and next words can be useful as informations. The Bidirectional took then the GRU. With an already heavy model, using GRU instead of a classical LSTM allowed for a sliglt decreasing number of computations and space."
      ],
      "metadata": {
        "id": "s8Rtisf_mOk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = Input(shape=(num_words, encoding_dim), name='text_input')\n",
        "cr = Bidirectional(GRU(128))(text_input)"
      ],
      "metadata": {
        "id": "YnSOe_5DKIlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, another input layer is defined for the other features. This input layer has 'num_features' dimensions (feature_shape), that is 26 , after the application of column transformer. This means that the MLP later will analyze each sample of 26 features out of the 560 samples for the training."
      ],
      "metadata": {
        "id": "qZVV_1oonxYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_shape = X_train_fearues_def_t.shape[1]\n",
        "feature_inputs = Input(shape=(feature_shape), name='feature_inputs')"
      ],
      "metadata": {
        "id": "o9Y3BfZyKx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that i have the output of the Recurrent 'cr' and the 'feature_inputs', i can concatenate them. This means that, by having my 'cr' of dimension 128 and the 'feature_inputs' of dimension 26, the MLP will take as input for each sample a 128+26 dimension vector.\n",
        "\n",
        "Then, there is the MLP. I precise that later there will be a dedicated section for the choice of each network parameter."
      ],
      "metadata": {
        "id": "O_sCB2LAoSJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.regularization.dropout import Dropout\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "# Concatenate the LSTM output with the other inputs\n",
        "concat = Concatenate()([cr, feature_inputs])\n",
        "\n",
        "# Define the MLP layers\n",
        "l1 = Dense(256, activation='sigmoid',kernel_initializer='glorot_normal')(concat)\n",
        "ld = Dropout(0.2)(l1)\n",
        "l2 = Dense(128, activation='swish',kernel_initializer='he_normal')(l1)\n",
        "l3 = Dense(64, activation='sigmoid',kernel_initializer='glorot_normal')(l2)\n",
        "output = Dense(num_classes, activation='softmax')(l3)"
      ],
      "metadata": {
        "id": "XrofY1j5LW6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the creation of the model. I create the model and pass to it a list of tensors instead of only one. This will allow for using each one in different moments (one before for the recurrent and the other later on in the MLP). As output of the model i take the output layer defined above after the MLP. According to what said in the exam, the output layer has num_classes neurons (5) and uses a softmax activation, so that the output is a vector with the probabilities of belonging to each class. (section 2 of the exam)\n",
        "\n",
        "Then, the loss i used was the SCCE, because we have continous class labels to predict. (part of section 5 of the exam)"
      ],
      "metadata": {
        "id": "ZF8iavNHo7kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "model = Model(inputs=[text_input, feature_inputs], outputs=output)"
      ],
      "metadata": {
        "id": "FyXQjUyyM55u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameters and evaluation**\n",
        "\n",
        "\n",
        "Until now i examined only the 'fixed' choiches, while in this section i will examine all the hyperparameters described in the written exam. This part corresponds to the requests 3-4-5-6 of the exam.\n",
        "As said in the exam, i took on a mininmalist approach. Initially the model had only the LSTM layer and a simple 3-layers MLP (output layer included). Then, as other hyperparameters i only used a 32 batch size and 30 epochs. At the actual state, here are the parameters and the reasons i choose them:\n",
        "\n",
        "- Bidirectional(GRU): i choosed this because of the reason previously indicated;\n",
        "- dimension of recurrent state (128): I started with a smaller value, then i saw that the performance was slighly increasing by increasing this dimension;\n",
        "- MLP: initially it had only 2 layers (output excluded) with few neurons. I verified that the network was benefitting from a little deep (3 dense layers before the output one) and expecially from an increased number of neurons (initally only 64, now the biggest layer has 256 of them, and the number decreases until 64 in the last layer). The activations in the MLP are most of all Sigmoids. In the written part i stated that Relu or Swish were better, but not in this extreme case. In fact, after all the processing, the input of the MLP is quite sparse (contains a lot of 0), so the problem of the dying Relu actually occured. For this reason i used sigmoid activations in the first and last layers, and swish in the second (if used only here, the performance wasn't affected). Then, the corresponding initializers were used, as said in the exam (GlorotNormal for Sigmoid and HeNormal for Swish).\n",
        "\n",
        "For the other parameters:\n",
        "\n",
        "- optimizer: i tried both Adam and SGD, and the first was, as predicted, better, and had also a faster training;\n",
        "- learning rate: initially i tried a small one (0.0001) and then tried bigger ones, but the best performance was obtained with a decaying one (in particular exponential schedule with the parameters above);\n",
        "- metrics: this choice was quite forced, in the case of multiclass classification the accuracy is the most informative one;\n",
        "- epochs: initially 30, but in all the cases i tried i saw a sort of plateau after 20 of them, so i left the value at 20 to not waste resources;\n",
        "- batch size: initially  i tought that a smaller batch size would be better, but by trying differenf of them i verified that a bigger one was improving the performances (and also speeding up the training).\n",
        "\n",
        "Then, by testing the model, i added two more things:\n",
        "\n",
        "- Dropout: i added a dropout(0.02) layer after the first dense;\n",
        "- Imbalance classes: i saw that the majority of the reviews had 5 stars. In fact, initially the loss was stacked in a local minima, assigning 5 stars to each review and reaching a low loss but with a not compatible performance (this was the major factor that made me think about this issue). I couldn't handle this with an oversmapling method, because the model has too many features, and also with an unsampling one, because there are already few training instances. The only reasonable solution i found was to assign a weight to each class. In this way the performance decreased a lot, but at least the model wasn't unbalanced anymore. I also reflected on the fact that, maybe, if the majority of the reviews have 5/5, maybe it was due to the fact that the park was particularly appreciated, so this sort of 'bias' wasn't cursing the model, but then i preferred to add the weights because in an ideal setting this would guarantee better generalization for other park reviews (that maybe aren't that great).\n",
        "\n",
        "Here below we can see the situation graphically, for the target vector y_train"
      ],
      "metadata": {
        "id": "zp9nTC3jrduM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.plot.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "7Jd2_YeHWS2c",
        "outputId": "b0185fd4-e65a-43ac-9829-ec3909b76edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Frequency'>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGdCAYAAAD0e7I1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAr80lEQVR4nO3df1TVdZ7H8ddFBMwExAYurPhjyt/m7yLSJk0mFMfVdDdtqcyYbGa0VPolZ0qzmlArMxsSawprJ8fJnXTLNspQYSo0RSlzXPyRCSUX2jW5gishfPePTnfnKhhcLtx7P/t8nPM9p/v5fr5f3h8+nsOrz/1877VZlmUJAADAUEG+LgAAAKAtEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYL9nUB/qChoUEnTpxQly5dZLPZfF0OAABoBsuydPr0acXFxSkoqOn1G8KOpBMnTig+Pt7XZQAAAA+UlZWpe/fuTZ4n7Ejq0qWLpO9/WeHh4T6uBgAANIfT6VR8fLzr73hTCDuS662r8PBwwg4AAAHmx7agsEEZAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGjBvvzhBQUFeuqpp1RUVKTy8nJt2rRJU6dOdetz8OBBPfTQQ8rPz9e5c+c0cOBA/eUvf1GPHj0kSWfPntV9992nDRs2qLa2VsnJyXrhhRcUExPjgxEBANC2ei16x9cltNiXyyb59Of7dGWnpqZGQ4cOVVZWVqPnjx49qjFjxqh///7asWOHPvvsMz3yyCMKCwtz9Vm4cKHefvttbdy4Ufn5+Tpx4oSmTZvWXkMAAAB+zqcrOxMnTtTEiRObPP/b3/5WKSkpWrFihavt8ssvd/13VVWVXn75Za1fv1433HCDJCknJ0cDBgzQzp07dc0117Rd8QAAICD47Z6dhoYGvfPOO+rbt6+Sk5MVHR2thIQEbd682dWnqKhIdXV1SkpKcrX1799fPXr0UGFhoQ+qBgAA/sZvw05lZaWqq6u1bNkyTZgwQe+//75uuukmTZs2Tfn5+ZIkh8OhkJAQRUZGul0bExMjh8PR5L1ra2vldDrdDgAAYCafvo11MQ0NDZKkKVOmaOHChZKkYcOG6eOPP1Z2drauv/56j++dmZmppUuXeqVOAADg3/x2Zeeyyy5TcHCwBg4c6NY+YMAAlZaWSpLsdru+++47nTp1yq1PRUWF7HZ7k/fOyMhQVVWV6ygrK/N6/QAAwD/4bdgJCQnRVVddpZKSErf2Q4cOqWfPnpKkkSNHqmPHjsrLy3OdLykpUWlpqRITE5u8d2hoqMLDw90OAABgJp++jVVdXa0jR464Xh87dkzFxcWKiopSjx499MADD2jGjBn62c9+pnHjxik3N1dvv/22duzYIUmKiIhQWlqa0tPTFRUVpfDwcN1zzz1KTEzkSSwAACDJx2Fnz549GjdunOt1enq6JGnWrFlat26dbrrpJmVnZyszM1P33nuv+vXrp7/85S8aM2aM65pnn31WQUFBmj59utuHCgIAAEiSzbIsy9dF+JrT6VRERISqqqp4SwsA4Nf4BOX/09y/3367ZwcAAMAbCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNF8GnYKCgo0efJkxcXFyWazafPmzU32/dWvfiWbzaZVq1a5tZ88eVKpqakKDw9XZGSk0tLSVF1d3baFAwCAgOHTsFNTU6OhQ4cqKyvrov02bdqknTt3Ki4u7oJzqampOnDggLZu3aotW7aooKBAc+bMaauSAQBAgAn25Q+fOHGiJk6ceNE+X3/9te655x699957mjRpktu5gwcPKjc3V7t379aoUaMkSc8//7xSUlL09NNPNxqOAADA/y9+vWenoaFBt912mx544AENGjTogvOFhYWKjIx0BR1JSkpKUlBQkHbt2tXkfWtra+V0Ot0OAABgJr8OO8uXL1dwcLDuvffeRs87HA5FR0e7tQUHBysqKkoOh6PJ+2ZmZioiIsJ1xMfHe7VuAADgP/w27BQVFem5557TunXrZLPZvHrvjIwMVVVVuY6ysjKv3h8AAPgPvw07f/3rX1VZWakePXooODhYwcHBOn78uO677z716tVLkmS321VZWel23blz53Ty5EnZ7fYm7x0aGqrw8HC3AwAAmMmnG5Qv5rbbblNSUpJbW3Jysm677TbNnj1bkpSYmKhTp06pqKhII0eOlCRt27ZNDQ0NSkhIaPeaAQCA//Fp2KmurtaRI0dcr48dO6bi4mJFRUWpR48e6tatm1v/jh07ym63q1+/fpKkAQMGaMKECbrrrruUnZ2turo6zZs3TzNnzuRJLAAAIMnHb2Pt2bNHw4cP1/DhwyVJ6enpGj58uBYvXtzse7z++uvq37+/xo8fr5SUFI0ZM0YvvvhiW5UMAAACjE9XdsaOHSvLsprd/8svv7ygLSoqSuvXr/diVQAAwCR+u0EZAADAGwg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACj+TTsFBQUaPLkyYqLi5PNZtPmzZtd5+rq6vTQQw/pyiuvVOfOnRUXF6fbb79dJ06ccLvHyZMnlZqaqvDwcEVGRiotLU3V1dXtPBIAAOCvfBp2ampqNHToUGVlZV1w7syZM9q7d68eeeQR7d27V2+++aZKSkr0j//4j279UlNTdeDAAW3dulVbtmxRQUGB5syZ015DAAAAfs5mWZbl6yIkyWazadOmTZo6dWqTfXbv3q2rr75ax48fV48ePXTw4EENHDhQu3fv1qhRoyRJubm5SklJ0VdffaW4uLhm/Wyn06mIiAhVVVUpPDzcG8MBAKBN9Fr0jq9LaLEvl01qk/s29+93QO3Zqaqqks1mU2RkpCSpsLBQkZGRrqAjSUlJSQoKCtKuXbt8VCUAAPAnwb4uoLnOnj2rhx56SLfccosrvTkcDkVHR7v1Cw4OVlRUlBwOR5P3qq2tVW1treu10+lsm6IBAIDPBcTKTl1dnW6++WZZlqU1a9a0+n6ZmZmKiIhwHfHx8V6oEgAA+CO/Dzs/BJ3jx49r69atbu/J2e12VVZWuvU/d+6cTp48Kbvd3uQ9MzIyVFVV5TrKysrarH4AAOBbfv021g9B5/Dhw9q+fbu6devmdj4xMVGnTp1SUVGRRo4cKUnatm2bGhoalJCQ0OR9Q0NDFRoa2qa1AwAA/+DTsFNdXa0jR464Xh87dkzFxcWKiopSbGys/umf/kl79+7Vli1bVF9f79qHExUVpZCQEA0YMEATJkzQXXfdpezsbNXV1WnevHmaOXNms5/EAgAAZvNp2NmzZ4/GjRvnep2eni5JmjVrlh599FG99dZbkqRhw4a5Xbd9+3aNHTtWkvT6669r3rx5Gj9+vIKCgjR9+nStXr26XeoHAAD+z6dhZ+zYsbrYx/w05yOAoqKitH79em+WBQAADOL3G5QBAABag7ADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYLRgXxcAADBDr0Xv+LqEFvty2SRfl4B2wMoOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMJpPw05BQYEmT56suLg42Ww2bd682e28ZVlavHixYmNj1alTJyUlJenw4cNufU6ePKnU1FSFh4crMjJSaWlpqq6ubsdRAAAAf+bTsFNTU6OhQ4cqKyur0fMrVqzQ6tWrlZ2drV27dqlz585KTk7W2bNnXX1SU1N14MABbd26VVu2bFFBQYHmzJnTXkMAAAB+LtiXP3zixImaOHFio+csy9KqVav08MMPa8qUKZKk1157TTExMdq8ebNmzpypgwcPKjc3V7t379aoUaMkSc8//7xSUlL09NNPKy4urt3GAgAA/JPf7tk5duyYHA6HkpKSXG0RERFKSEhQYWGhJKmwsFCRkZGuoCNJSUlJCgoK0q5du5q8d21trZxOp9sBAADM5FHY+eKLL7xdxwUcDockKSYmxq09JibGdc7hcCg6OtrtfHBwsKKiolx9GpOZmamIiAjXER8f7+XqAQCAv/Ao7FxxxRUaN26c/vjHP7rtnwkUGRkZqqqqch1lZWW+LgkAALQRj8LO3r17NWTIEKWnp8tut+vuu+/WJ5984tXC7Ha7JKmiosKtvaKiwnXObrersrLS7fy5c+d08uRJV5/GhIaGKjw83O0AAABm8ijsDBs2TM8995xOnDihV155ReXl5RozZowGDx6slStX6ptvvml1Yb1795bdbldeXp6rzel0ateuXUpMTJQkJSYm6tSpUyoqKnL12bZtmxoaGpSQkNDqGgAAQOBr1Qbl4OBgTZs2TRs3btTy5ct15MgR3X///YqPj9ftt9+u8vLyi15fXV2t4uJiFRcXS/p+U3JxcbFKS0tls9m0YMECPfHEE3rrrbe0f/9+3X777YqLi9PUqVMlSQMGDNCECRN011136ZNPPtFHH32kefPmaebMmTyJBQAAJLUy7OzZs0e/+c1vFBsbq5UrV+r+++/X0aNHtXXrVp04ccL1yPjFrh8+fLiGDx8uSUpPT9fw4cO1ePFiSdKDDz6oe+65R3PmzNFVV12l6upq5ebmKiwszHWP119/Xf3799f48eOVkpKiMWPG6MUXX2zNsAAAgEFslmVZLb1o5cqVysnJUUlJiVJSUvTLX/5SKSkpCgr6v+z01VdfqVevXjp37pxXC24LTqdTERERqqqqYv8OAHio16J3fF1Ci325bJKvS2gxfs//p7l/vz36UME1a9bozjvv1B133KHY2NhG+0RHR+vll1/25PYAAABe41HYOf/7qRoTEhKiWbNmeXJ7AAAAr/Foz05OTo42btx4QfvGjRv16quvtrooAAAAb/Eo7GRmZuqyyy67oD06OlpPPvlkq4sCAADwFo/CTmlpqXr37n1Be8+ePVVaWtrqogAAALzFo7ATHR2tzz777IL2Tz/9VN26dWt1UQAAAN7iUdi55ZZbdO+992r79u2qr69XfX29tm3bpvnz52vmzJnerhEAAMBjHj2N9fjjj+vLL7/U+PHjFRz8/S0aGhp0++23s2cHAAD4FY/CTkhIiP785z/r8ccf16effqpOnTrpyiuvVM+ePb1dHwAAQKt4FHZ+0LdvX/Xt29dbtQAAAHidR2Gnvr5e69atU15eniorK9XQ0OB2ftu2bV4pDgAAoLU8Cjvz58/XunXrNGnSJA0ePFg2m83bdQEAAHiFR2Fnw4YNeuONN5SSkuLtegAAALzKo0fPQ0JCdMUVV3i7FgAAAK/zKOzcd999eu6552RZlrfrAQAA8CqP3sb68MMPtX37dr377rsaNGiQOnbs6Hb+zTff9EpxAAAAreVR2ImMjNRNN93k7VoAAAC8zqOwk5OT4+06AAAA2oRHe3Yk6dy5c/rggw+0du1anT59WpJ04sQJVVdXe604AACA1vJoZef48eOaMGGCSktLVVtbq5///Ofq0qWLli9frtraWmVnZ3u7TgAAAI94tLIzf/58jRo1St9++606derkar/pppuUl5fnteIAAABay6OVnb/+9a/6+OOPFRIS4tbeq1cvff31114pDAAAwBs8WtlpaGhQfX39Be1fffWVunTp0uqiAAAAvMWjsHPjjTdq1apVrtc2m03V1dVasmQJXyEBAAD8ikdvYz3zzDNKTk7WwIEDdfbsWf3Lv/yLDh8+rMsuu0x/+tOfvF0jAACAxzwKO927d9enn36qDRs26LPPPlN1dbXS0tKUmprqtmEZAADA1zwKO5IUHBysW2+91Zu1AAAAeJ1HYee111676Pnbb7/do2IAAAC8zaOwM3/+fLfXdXV1OnPmjEJCQnTJJZcQdgAAgN/w6Gmsb7/91u2orq5WSUmJxowZwwZlAADgVzz+bqzz9enTR8uWLbtg1QcAAMCXvBZ2pO83LZ84ccKbtwQAAGgVj/bsvPXWW26vLctSeXm5fv/732v06NFeKUyS6uvr9eijj+qPf/yjHA6H4uLidMcdd+jhhx+WzWZz/ewlS5bopZde0qlTpzR69GitWbNGffr08VodAAAgcHkUdqZOner22maz6Sc/+YluuOEGPfPMM96oS5K0fPlyrVmzRq+++qoGDRqkPXv2aPbs2YqIiNC9994rSVqxYoVWr16tV199Vb1799Yjjzyi5ORk/e1vf1NYWJjXagEAAIHJo7DT0NDg7Toa9fHHH2vKlCmaNGmSpO+/aPRPf/qTPvnkE0nfr+qsWrVKDz/8sKZMmSLp+8fiY2JitHnzZs2cObNd6gQAAP7Lq3t2vO3aa69VXl6eDh06JEn69NNP9eGHH2rixImSpGPHjsnhcCgpKcl1TUREhBISElRYWNjkfWtra+V0Ot0OAABgJo9WdtLT05vdd+XKlZ78CEnSokWL5HQ61b9/f3Xo0EH19fX63e9+p9TUVEmSw+GQJMXExLhdFxMT4zrXmMzMTC1dutTjugAAQODwKOzs27dP+/btU11dnfr16ydJOnTokDp06KARI0a4+v2widhTb7zxhl5//XWtX79egwYNUnFxsRYsWKC4uDjNmjXL4/tmZGS4BTan06n4+PhW1QoAAPyTR2Fn8uTJ6tKli1599VV17dpV0vcfNDh79mxdd911uu+++7xS3AMPPKBFixa59t5ceeWVOn78uDIzMzVr1izZ7XZJUkVFhWJjY13XVVRUaNiwYU3eNzQ0VKGhoV6pEQAA+DeP9uw888wzyszMdAUdSerataueeOIJrz6NdebMGQUFuZfYoUMH1wbp3r17y263Ky8vz3Xe6XRq165dSkxM9FodAAAgcHm0suN0OvXNN99c0P7NN9/o9OnTrS7qB5MnT9bvfvc79ejRQ4MGDdK+ffu0cuVK3XnnnZK+f5tswYIFeuKJJ9SnTx/Xo+dxcXEXPB4PAAD+f/Io7Nx0002aPXu2nnnmGV199dWSpF27dumBBx7QtGnTvFbc888/r0ceeUS/+c1vVFlZqbi4ON19991avHixq8+DDz6ompoazZkzR6dOndKYMWOUm5vLZ+wAAABJks2yLKulF505c0b333+/XnnlFdXV1Un6/qsi0tLS9NRTT6lz585eL7QtOZ1ORUREqKqqSuHh4b4uBwACUq9F7/i6hBb7ctkkX5fQYvye/09z/357tLJzySWX6IUXXtBTTz2lo0ePSpIuv/zygAs5AADAfK36UMHy8nKVl5erT58+6ty5szxYJAIAAGhTHoWd//7v/9b48ePVt29fpaSkqLy8XJKUlpbmtcfOAQAAvMGjsLNw4UJ17NhRpaWluuSSS1ztM2bMUG5urteKAwAAaC2P9uy8//77eu+999S9e3e39j59+uj48eNeKQwAAMAbPFrZqampcVvR+cHJkyf5ZGIAAOBXPAo71113nV577TXXa5vNpoaGBq1YsULjxo3zWnEAAACt5dHbWCtWrND48eO1Z88efffdd3rwwQd14MABnTx5Uh999JG3awQAAPCYRys7gwcP1qFDhzRmzBhNmTJFNTU1mjZtmvbt26fLL7/c2zUCAAB4rMUrO3V1dZowYYKys7P129/+ti1qAgAA8JoWr+x07NhRn332WVvUAgAA4HUevY1166236uWXX/Z2LQAAAF7n0Qblc+fO6ZVXXtEHH3ygkSNHXvCdWCtXrvRKcQAAAK3VorDzxRdfqFevXvr88881YsQISdKhQ4fc+thsNu9VBwAA0EotCjt9+vRReXm5tm/fLun7r4dYvXq1YmJi2qQ4AACA1mrRnp3zv9X83XffVU1NjVcLAgAA8CaPNij/4PzwAwAA4G9aFHZsNtsFe3LYowMAAPxZi/bsWJalO+64w/Vln2fPntWvfvWrC57GevPNN71XIQAAQCu0KOzMmjXL7fWtt97q1WIAAAC8rUVhJycnp63qAAAAaBOt2qAMAADg7wg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADCa34edr7/+Wrfeequ6deumTp066corr9SePXtc5y3L0uLFixUbG6tOnTopKSlJhw8f9mHFAADAn/h12Pn22281evRodezYUe+++67+9re/6ZlnnlHXrl1dfVasWKHVq1crOztbu3btUufOnZWcnKyzZ8/6sHIAAOAvgn1dwMUsX75c8fHxysnJcbX17t3b9d+WZWnVqlV6+OGHNWXKFEnSa6+9ppiYGG3evFkzZ85s95oBAIB/8euVnbfeekujRo3SP//zPys6OlrDhw/XSy+95Dp/7NgxORwOJSUludoiIiKUkJCgwsLCJu9bW1srp9PpdgAAADP5ddj54osvtGbNGvXp00fvvfeefv3rX+vee+/Vq6++KklyOBySpJiYGLfrYmJiXOcak5mZqYiICNcRHx/fdoMAAAA+5ddhp6GhQSNGjNCTTz6p4cOHa86cObrrrruUnZ3dqvtmZGSoqqrKdZSVlXmpYgAA4G/8OuzExsZq4MCBbm0DBgxQaWmpJMlut0uSKioq3PpUVFS4zjUmNDRU4eHhbgcAADCTX4ed0aNHq6SkxK3t0KFD6tmzp6TvNyvb7Xbl5eW5zjudTu3atUuJiYntWisAAPBPfv001sKFC3XttdfqySef1M0336xPPvlEL774ol588UVJks1m04IFC/TEE0+oT58+6t27tx555BHFxcVp6tSpvi0eAAD4Bb8OO1dddZU2bdqkjIwMPfbYY+rdu7dWrVql1NRUV58HH3xQNTU1mjNnjk6dOqUxY8YoNzdXYWFhPqwcAAD4C78OO5L0i1/8Qr/4xS+aPG+z2fTYY4/psccea8eqAABAoPDrPTsAAACtRdgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMFqwrwsAgLbWa9E7vi6hxb5cNsnXJQDGYGUHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMFpAhZ1ly5bJZrNpwYIFrrazZ89q7ty56tatmy699FJNnz5dFRUVvisSAAD4lYAJO7t379batWs1ZMgQt/aFCxfq7bff1saNG5Wfn68TJ05o2rRpPqoSAAD4m4AIO9XV1UpNTdVLL72krl27utqrqqr08ssva+XKlbrhhhs0cuRI5eTk6OOPP9bOnTt9WDEAAPAXARF25s6dq0mTJikpKcmtvaioSHV1dW7t/fv3V48ePVRYWNjk/Wpra+V0Ot0OAABgpmBfF/BjNmzYoL1792r37t0XnHM4HAoJCVFkZKRbe0xMjBwOR5P3zMzM1NKlS71dKgAA8EN+vbJTVlam+fPn6/XXX1dYWJjX7puRkaGqqirXUVZW5rV7AwAA/+LXYaeoqEiVlZUaMWKEgoODFRwcrPz8fK1evVrBwcGKiYnRd999p1OnTrldV1FRIbvd3uR9Q0NDFR4e7nYAAAAz+fXbWOPHj9f+/fvd2mbPnq3+/fvroYceUnx8vDp27Ki8vDxNnz5dklRSUqLS0lIlJib6omQAAOBn/DrsdOnSRYMHD3Zr69y5s7p16+ZqT0tLU3p6uqKiohQeHq577rlHiYmJuuaaa3xRMgAA8DN+HXaa49lnn1VQUJCmT5+u2tpaJScn64UXXvB1WQAAwE8EXNjZsWOH2+uwsDBlZWUpKyvLNwUBAAC/5tcblAEAAFqLsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMFqwrwsAvKXXond8XUKLfblskq9LAADj+f3KTmZmpq666ip16dJF0dHRmjp1qkpKStz6nD17VnPnzlW3bt106aWXavr06aqoqPBRxQAAwJ/4fdjJz8/X3LlztXPnTm3dulV1dXW68cYbVVNT4+qzcOFCvf3229q4caPy8/N14sQJTZs2zYdVAwAAf+H3b2Pl5ua6vV63bp2io6NVVFSkn/3sZ6qqqtLLL7+s9evX64YbbpAk5eTkaMCAAdq5c6euueYaX5QNAAD8hN+v7JyvqqpKkhQVFSVJKioqUl1dnZKSklx9+vfvrx49eqiwsLDRe9TW1srpdLodAADATAEVdhoaGrRgwQKNHj1agwcPliQ5HA6FhIQoMjLSrW9MTIwcDkej98nMzFRERITriI+Pb+vSAQCAjwRU2Jk7d64+//xzbdiwoVX3ycjIUFVVlesoKyvzUoUAAMDf+P2enR/MmzdPW7ZsUUFBgbp37+5qt9vt+u6773Tq1Cm31Z2KigrZ7fZG7xUaGqrQ0NC2LhkAAPgBv1/ZsSxL8+bN06ZNm7Rt2zb17t3b7fzIkSPVsWNH5eXludpKSkpUWlqqxMTE9i4XAAD4Gb9f2Zk7d67Wr1+vf//3f1eXLl1c+3AiIiLUqVMnRUREKC0tTenp6YqKilJ4eLjuueceJSYm8iQWAADw/7CzZs0aSdLYsWPd2nNycnTHHXdIkp599lkFBQVp+vTpqq2tVXJysl544YV2rhQAAPgjvw87lmX9aJ+wsDBlZWUpKyurHSoCAACBxO/37AAAALQGYQcAABjN79/GCnR8EzcAAL7Fyg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMZE3aysrLUq1cvhYWFKSEhQZ988omvSwIAAH7AiLDz5z//Wenp6VqyZIn27t2roUOHKjk5WZWVlb4uDQAA+JgRYWflypW66667NHv2bA0cOFDZ2dm65JJL9Morr/i6NAAA4GPBvi6gtb777jsVFRUpIyPD1RYUFKSkpCQVFhY2ek1tba1qa2tdr6uqqiRJTqfT6/U11J7x+j3bWlv8HtoDv2s0hX8b7YPfc/vg93zhfS3Lumi/gA87//Vf/6X6+nrFxMS4tcfExOg///M/G70mMzNTS5cuvaA9Pj6+TWoMNBGrfF3B/x/8rtEU/m20D37P7aOtf8+nT59WREREk+cDPux4IiMjQ+np6a7XDQ0NOnnypLp16yabzea1n+N0OhUfH6+ysjKFh4d77b7+xPQxMr7AZ/oYTR+fZP4YGZ/nLMvS6dOnFRcXd9F+AR92LrvsMnXo0EEVFRVu7RUVFbLb7Y1eExoaqtDQULe2yMjItipR4eHhRv4D/numj5HxBT7Tx2j6+CTzx8j4PHOxFZ0fBPwG5ZCQEI0cOVJ5eXmutoaGBuXl5SkxMdGHlQEAAH8Q8Cs7kpSenq5Zs2Zp1KhRuvrqq7Vq1SrV1NRo9uzZvi4NAAD4mBFhZ8aMGfrmm2+0ePFiORwODRs2TLm5uRdsWm5voaGhWrJkyQVvmZnE9DEyvsBn+hhNH59k/hgZX9uzWT/2vBYAAEAAC/g9OwAAABdD2AEAAEYj7AAAAKMRdgAAgNEIO61QUFCgyZMnKy4uTjabTZs3b/7Ra3bs2KERI0YoNDRUV1xxhdatW9fmdXqqpePbsWOHbDbbBYfD4WifglsoMzNTV111lbp06aLo6GhNnTpVJSUlP3rdxo0b1b9/f4WFhenKK6/Uf/zHf7RDtS3nyfjWrVt3wfyFhYW1U8Utt2bNGg0ZMsT1YWWJiYl69913L3pNoMyf1PLxBdr8nW/ZsmWy2WxasGDBRfsF0hyerzljDKR5fPTRRy+otX///he9xhfzR9hphZqaGg0dOlRZWVnN6n/s2DFNmjRJ48aNU3FxsRYsWKBf/vKXeu+999q4Us+0dHw/KCkpUXl5ueuIjo5uowpbJz8/X3PnztXOnTu1detW1dXV6cYbb1RNTU2T13z88ce65ZZblJaWpn379mnq1KmaOnWqPv/883asvHk8GZ/0/aec/v38HT9+vJ0qbrnu3btr2bJlKioq0p49e3TDDTdoypQpOnDgQKP9A2n+pJaPTwqs+ft7u3fv1tq1azVkyJCL9gu0Ofx7zR2jFFjzOGjQILdaP/zwwyb7+mz+LHiFJGvTpk0X7fPggw9agwYNcmubMWOGlZyc3IaVeUdzxrd9+3ZLkvXtt9+2S03eVllZaUmy8vPzm+xz8803W5MmTXJrS0hIsO6+++62Lq/VmjO+nJwcKyIiov2KagNdu3a1/vCHPzR6LpDn7wcXG1+gzt/p06etPn36WFu3brWuv/56a/78+U32DdQ5bMkYA2kelyxZYg0dOrTZ/X01f6zstKPCwkIlJSW5tSUnJ6uwsNBHFbWNYcOGKTY2Vj//+c/10Ucf+bqcZquqqpIkRUVFNdknkOewOeOTpOrqavXs2VPx8fE/uorgT+rr67VhwwbV1NQ0+VUxgTx/zRmfFJjzN3fuXE2aNOmCuWlMoM5hS8YoBdY8Hj58WHFxcfrpT3+q1NRUlZaWNtnXV/NnxCcoBwqHw3HBpzrHxMTI6XTqf/7nf9SpUycfVeYdsbGxys7O1qhRo1RbW6s//OEPGjt2rHbt2qURI0b4uryLamho0IIFCzR69GgNHjy4yX5NzaG/7kv6QXPH169fP73yyisaMmSIqqqq9PTTT+vaa6/VgQMH1L1793asuPn279+vxMREnT17Vpdeeqk2bdqkgQMHNto3EOevJeMLxPnbsGGD9u7dq927dzerfyDOYUvHGEjzmJCQoHXr1qlfv34qLy/X0qVLdd111+nzzz9Xly5dLujvq/kj7MBr+vXrp379+rleX3vttTp69KieffZZ/eu//qsPK/txc+fO1eeff37R95oDWXPHl5iY6LZqcO2112rAgAFau3atHn/88bYu0yP9+vVTcXGxqqqq9G//9m+aNWuW8vPzmwwEgaYl4wu0+SsrK9P8+fO1detWv92A21qejDGQ5nHixImu/x4yZIgSEhLUs2dPvfHGG0pLS/NhZe4IO+3IbreroqLCra2iokLh4eEBv6rTlKuvvtrvA8S8efO0ZcsWFRQU/Oj/NTU1h3a7vS1LbJWWjO98HTt21PDhw3XkyJE2qq71QkJCdMUVV0iSRo4cqd27d+u5557T2rVrL+gbiPPXkvGdz9/nr6ioSJWVlW4rv/X19SooKNDvf/971dbWqkOHDm7XBNocejLG8/n7PP69yMhI9e3bt8lafTV/7NlpR4mJicrLy3Nr27p160Xffw90xcXFio2N9XUZjbIsS/PmzdOmTZu0bds29e7d+0evCaQ59GR856uvr9f+/fv9dg4b09DQoNra2kbPBdL8NeVi4zufv8/f+PHjtX//fhUXF7uOUaNGKTU1VcXFxY2GgECbQ0/GeD5/n8e/V11draNHjzZZq8/mr023Pxvu9OnT1r59+6x9+/ZZkqyVK1da+/bts44fP25ZlmUtWrTIuu2221z9v/jiC+uSSy6xHnjgAevgwYNWVlaW1aFDBys3N9dXQ7iolo7v2WeftTZv3mwdPnzY2r9/vzV//nwrKCjI+uCDD3w1hIv69a9/bUVERFg7duywysvLXceZM2dcfW677TZr0aJFrtcfffSRFRwcbD399NPWwYMHrSVLllgdO3a09u/f74shXJQn41u6dKn13nvvWUePHrWKioqsmTNnWmFhYdaBAwd8MYQftWjRIis/P986duyY9dlnn1mLFi2ybDab9f7771uWFdjzZ1ktH1+gzV9jzn9SKdDnsDE/NsZAmsf77rvP2rFjh3Xs2DHro48+spKSkqzLLrvMqqystCzLf+aPsNMKPzxqff4xa9Ysy7Isa9asWdb1119/wTXDhg2zQkJCrJ/+9KdWTk5Ou9fdXC0d3/Lly63LL7/cCgsLs6KioqyxY8da27Zt803xzdDY2CS5zcn111/vGu8P3njjDatv375WSEiINWjQIOudd95p38KbyZPxLViwwOrRo4cVEhJixcTEWCkpKdbevXvbv/hmuvPOO62ePXtaISEh1k9+8hNr/PjxriBgWYE9f5bV8vEF2vw15vwgEOhz2JgfG2MgzeOMGTOs2NhYKyQkxPqHf/gHa8aMGdaRI0dc5/1l/myWZVltu3YEAADgO+zZAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBo/wstx4nTJX5iPAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Discarded Choices**\n",
        "\n",
        "Also other parameters were tested, but then neglected for some reasons, here are them:\n",
        "\n",
        "- Batch Normalization: i tried it in some positions between the layers, but in every case it killed the performance;\n",
        "\n",
        "- Relu as activation: a little explained above, but shortly because of the sparsity there was dying Relu;\n",
        "\n",
        "- weight regularization: just tested with and without, the performance was quite the same, so i didn't used them to save space;\n",
        "\n",
        "\n",
        "Here below there are such the parameters involved in the .compile and .fit method. The ones involving layers are above and displayed in the summary below"
      ],
      "metadata": {
        "id": "78feFcXJUmDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classWeight = compute_class_weight('balanced', classes=np.unique(y_traindef), y=y_traindef)\n",
        "classWeight = dict(enumerate(classWeight))\n",
        "initial_learning_rate = 0.1\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.66,\n",
        "    staircase=True)\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "#class_weight=classWeight\n",
        "model.fit([X_train_reviews_tensor,X_train_fearues_def_t], y_train_dense, epochs=20, batch_size=100,class_weight=classWeight, validation_data=([X_test_reviews_tensor,X_test_fearues_def_t],y_test_dense))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwbLs4dQVdeR",
        "outputId": "5338d551-1c5b-4c73-e0ea-dc7d2efd3bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 16s 749ms/step - loss: 3.4250 - accuracy: 0.2111 - val_loss: 2.1036 - val_accuracy: 0.0444\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 1s 400ms/step - loss: 2.0790 - accuracy: 0.1722 - val_loss: 1.5657 - val_accuracy: 0.3111\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 1s 404ms/step - loss: 1.9425 - accuracy: 0.1056 - val_loss: 1.8967 - val_accuracy: 0.0556\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 1s 389ms/step - loss: 1.8068 - accuracy: 0.2778 - val_loss: 1.8331 - val_accuracy: 0.1444\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 1s 385ms/step - loss: 1.8262 - accuracy: 0.1306 - val_loss: 1.6057 - val_accuracy: 0.4444\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 1s 396ms/step - loss: 1.6694 - accuracy: 0.2750 - val_loss: 1.7630 - val_accuracy: 0.0556\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 1s 391ms/step - loss: 1.7179 - accuracy: 0.0917 - val_loss: 1.5700 - val_accuracy: 0.0444\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 1.6576 - accuracy: 0.3444 - val_loss: 1.6441 - val_accuracy: 0.0556\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 2s 439ms/step - loss: 1.6341 - accuracy: 0.0389 - val_loss: 1.6526 - val_accuracy: 0.1444\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 1s 390ms/step - loss: 1.6335 - accuracy: 0.2389 - val_loss: 1.5526 - val_accuracy: 0.3111\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 1s 410ms/step - loss: 1.6689 - accuracy: 0.1806 - val_loss: 1.6561 - val_accuracy: 0.1444\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 1s 395ms/step - loss: 1.6531 - accuracy: 0.0889 - val_loss: 1.5029 - val_accuracy: 0.4444\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 1s 400ms/step - loss: 1.6476 - accuracy: 0.3000 - val_loss: 1.7448 - val_accuracy: 0.0444\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 1s 397ms/step - loss: 1.6649 - accuracy: 0.2139 - val_loss: 1.6088 - val_accuracy: 0.0556\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 1s 394ms/step - loss: 1.6447 - accuracy: 0.2417 - val_loss: 1.6663 - val_accuracy: 0.0444\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 1s 392ms/step - loss: 1.6639 - accuracy: 0.3333 - val_loss: 1.6323 - val_accuracy: 0.0556\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 1s 388ms/step - loss: 1.7305 - accuracy: 0.0611 - val_loss: 1.5890 - val_accuracy: 0.3111\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 1s 393ms/step - loss: 1.6891 - accuracy: 0.2528 - val_loss: 1.5176 - val_accuracy: 0.1444\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 1s 398ms/step - loss: 1.6695 - accuracy: 0.1583 - val_loss: 1.8329 - val_accuracy: 0.0556\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 1s 372ms/step - loss: 1.6598 - accuracy: 0.1139 - val_loss: 1.5081 - val_accuracy: 0.1444\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ed8f13916f0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the model, beyond the mentioned accuracy, i also decided to use a validation set, mainly to encourage better generalization and discourage overfitting. In fact, in the .fit method, i added the test sets for both text and features inputs as validation sets, togheter with the y_test.\n",
        "From the results of the .fit method, we can see that the loss is quite low for both the training and validation part, but the accuracy is totally not reasonable. Also, the value of this accuracy is not following some sort of pattern (decreasing or increasing constantly), but it's jumping between some recurrent values. This suggests me that the model had found some local minima for the loss (as it is low), but the predictions are still totally inaccurate. Personally, i would not address all the falut to the architecture. In fact, the problem is that in the actual setting (better described in the next section) it is not even so sensitive to perform hyperparameter tuning. In fact, if the performance is so low, some very different combinations of hyperparameters could appear as quite equal or similar, while in other conditions no. But, the combination of parameters that i presented was still the best one, even tought it's still far from optimal it was better than the others."
      ],
      "metadata": {
        "id": "8FaaDTvdMV5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "M9isejA0J_dh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20b6ad9-6e34-4a2e-95c7-d4f72da3588a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text_input (InputLayer)        [(None, 130, 5466)]  0           []                               \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 256)          4297728     ['text_input[0][0]']             \n",
            "                                                                                                  \n",
            " feature_inputs (InputLayer)    [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 282)          0           ['bidirectional[0][0]',          \n",
            "                                                                  'feature_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          72448       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          32896       ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           8256        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 5)            325         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,411,653\n",
            "Trainable params: 4,411,653\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final evaluation on the performance**\n",
        "\n",
        "As it can be seen, the performance of the model is quite ridicolous. I would address this high bias, in importance order, to:\n",
        "\n",
        "- Word embedding: A more appropriate word embedding could have made the absolute difference in this model. The fact was that the only one treated in this course was the one-hot. If i had to choose i probably had used the pre-trained GloVe from Standford, which allows also to choose the dimension of the embedding. Another possibility (more computationally expensive) could had been to train a word2vec on the dictionary given by the corpus of all the reviews. The biggest problem of the one-hot encoding is that it is really space consuming.\n",
        "\n",
        "- Few samples: this problem is a consequence of the above one, in fact i was forced by the extreme dimension of the embedding to choose only a small subset of all the samples;\n",
        "\n",
        "For these reasons the purposed model, even if it has a good architecture (i think), it can't perform well."
      ],
      "metadata": {
        "id": "4J7HrSWTx5Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Possible improvements**\n",
        "\n",
        "The essential part of this dataset is the text one, and the other part i think it's already well processed. Also the architecture seems to be right. The biggest possible improvement would be to use some pre trained embeddings. Consequently it would be possible to use all the training set and even perform an automatic tuning of hyperparameters (not by hand as in this case). Another interesting thing to test would be to create a sort of 'score' for each review. The main idea is to compute the entropy (intended as quantity of information here) of each word, and then from it to compute the one of each review, in order to keep also another potentially important information."
      ],
      "metadata": {
        "id": "Ax7E_raG1Ti_"
      }
    }
  ]
}